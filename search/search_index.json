{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"background/","title":"Background","text":"<p>This section provides background on the Privacy Fingerprint (PrivFp) project, including a decision log and useful links.</p>"},{"location":"background/decision-log/","title":"Decision Log","text":""},{"location":"background/decision-log/#decisions","title":"Decisions","text":"<p>Here we collect decisions made throughout the project, focused on tooling, techniques, and frameworks - any limitations associated with such choices we attempt to catalogue alongside.</p> <ul> <li>19/03/24 - Made a decision log </li> <li>...</li> </ul>"},{"location":"background/privfp-background/","title":"Privacy Fingerprint","text":"<p>The aim of the Privacy Fingerprint (PrivFp) project is to develop a modular tool that could be used to calculate a privacy risk score on unstructured clinical data.</p>"},{"location":"background/privfp-background/#links","title":"Links","text":"<ul> <li>Original proof of concept repository</li> <li>Experiments repository</li> <li>Unstructured report repository</li> </ul>"},{"location":"background/privfp-background/#related-resources","title":"Related resources","text":"<ul> <li>Exploring Language Modelling for (NHS) Patient Safety Incident Reports</li> </ul>"},{"location":"correct-match/","title":"(Py)Correct Match (Scorer Module)","text":"<p>This page provides a short summary of what (Py)CorrectMatch is and how it may be used to extract out information on the uniqueness of individual records, and the global uniqueness across a whole dataset.</p>"},{"location":"correct-match/how-correct-match-works/","title":"How does Correct Match Work?","text":""},{"location":"correct-match/how-correct-match-works/#1extract-the-empirical-marginal-distribution-from-each-feature","title":"1.Extract the Empirical Marginal Distribution from each feature.","text":"<ul> <li>A variety of marginal distributions are fitted to the feature data:<ul> <li>Negative Binomial</li> <li>Geometric</li> <li>Categorical (probability of frequency counts)</li> <li>Logarithmic</li> </ul> </li> <li>Bayesian Information Criterion (BIC) is used to determine the best fitting distirbution.</li> </ul>"},{"location":"correct-match/how-correct-match-works/#2extract-the-correlation-matrix-between-features","title":"2.Extract the Correlation Matrix between features","text":""},{"location":"correct-match/how-correct-match-works/#3fit-a-gaussian-copula-using-the-estimated-marginal-distributions","title":"3.Fit a Gaussian Copula using the estimated Marginal Distributions","text":"<ul> <li>Gaussian Copula is fitted to the dataset using maximum likelihood estimation (MLE).</li> <li>An optimization procedure is performed to fit the correlation matrix of the Gaussian Copula. The optimization aims to minimize the difference between the calculated mutual information and the target mutual information.</li> </ul>"},{"location":"correct-match/how-correct-match-works/#4analogy-to-summarise-how-a-fitted-gaussian-copula-understands-uniqueness-of-individuals-and-across-a-whole-dataset","title":"4.Analogy to summarise how a fitted Gaussian Copula understands uniqueness of individuals and across a whole dataset.","text":"<ul> <li>Gaussian Copula Model: Imagine you have a recipe for cookies that includes various ingredients like flour, sugar, chocolate chips, etc. Each ingredient represents a variable in your dataset, and the recipe captures the correlations between these ingredients (variables).</li> <li>Generate Random Samples: Now, instead of using exact measurements from the recipe, you randomly adjust the quantities of ingredients within reasonable ranges, maintaining the overall proportions specified by the recipe. These randomly adjusted ingredient quantities represent the random samples generated from the Gaussian copula model.</li> <li>Calculate Cell Probabilities: Estimating cell probabilities involves determining the likelihood of an individual data point (or in our analogy, a batch of cookies with specific ingredient quantities) falling into each combination of marginals within the joint distribution modelled by the Gaussian copula. Based on these observations, you estimate the probability of each combination of ingredient quantities producing the desired cookies. </li> </ul> <p>Similarly, in the data context, the smooth_weight function estimates the probability of each combination of discrete values for an individual's data based on the random samples generated from the Gaussian copula model.</p> <p></p>"},{"location":"correct-match/what-is-correct-match/","title":"What is (Py)Correct Match?","text":"<p>Correct Match is a Julia module consisting of several components for analysing data uniqueness using a Gaussian copula model.</p> <p>Top Level Overview: Gaussian copula model is trained across the whole dataframe, then this model is used to assess and extract the \"uniqueness\" score for each individual data point. </p> <p></p>"},{"location":"open-source-extraction-exploration/","title":"Open source Extraction Exploration","text":"<p>Once we have LLM-generated medical notes we then want to extract entities from these notes to then produce a privacy risk score. </p> <p>In previous work, AWS Comprehend Medical was first used to extract entities from these medical notes. In this project we want to explore using open-source named-entity extraction methods that could be used instead of AWS Comprehend Medical.</p> <p>Experiments and example notebooks for the extraction component of PrivFp are available in the privfp-experiments repository.</p>"},{"location":"open-source-extraction-exploration/deidentification_research/","title":"De-Identification Methods","text":"<p>Deidentification techniques refer to methods used to remove or mask personally identifiable information (PII) from data, while still retaining its utility for analysis or other purposes. </p> <p>De-Identification Methods</p> PresidoAnonCATMASKCloud Data Loss Prevention API <p>Presido is a de-identification SDK owned by microsoft.</p> <ul> <li>Regex to recognise patterns.</li> <li>Use Named-entity recognition model (default is set to en_core_web_lg and supports any Spacy model.)</li> <li>Validating patterns</li> <li>Uses context to increase detection confidence.</li> </ul> <p>AnonCAT is a transformer based approach fo redacting text from electronic health records.</p> <p>It uses a NER model, en_core_sci_md, to detect all medical terms. Then they assign each entitity to an ID in a biomedical databases (UMLS) to normalise the outputs. (decipher new diagnosis, history, or reason for admission.)</p> <p>MASK is Manchester University de-identification framework for named-entitity-recognition.</p> <ul> <li>BiLSTM layer - essentially considers each of the entities (beginning, in the middle, no entity) - and assigns a confidence value to help determine it's label across.</li> <li>CRFs Layer use the observed data to predict the labels of the sequence, while taking into account the dependencies between neighbouring labels. (Conditional Random Field)</li> <li>GLoVe embeddings - determines a word vector space that incorporates both the local context of words but also their co-occurence with other words across the space. </li> <li>ELMo embeddings - considers where words have the same spelling but different meaning (Polysemy.)  - takes the word representations and then take the entire input sentence into equation for calculating the word embeddings.</li> </ul> <p>DLP API is googles API for detection of privacy-sensitive fragments in text, images, and Google Cloud Platform storage repositories.</p> <p>There is a range of techniques that are implemented in this API, some noticeable ones being:</p> <ul> <li>Using basic RegexMatching for some PID data. etc. phone numbers.</li> <li>Using a hotword rule to instruct Sensitive Data Protection to adjust the likelihood of a finding, depending on whether a hotword occurs near that finding.</li> <li>Using exclusion rules to exclude false or unwanted finding (identifying custom substrings within a string.) - for example name inside an email.(Take the email, and not the name)</li> </ul>"},{"location":"open-source-extraction-exploration/named_entity_research/","title":"Open-source Named-entity Recognition Models","text":"<p>Named-entity Recognition (NER) models aim to label words/phrases in unstructured data as a classified entity.</p>"},{"location":"open-source-extraction-exploration/named_entity_research/#named-entity-recognition-models","title":"Named-entity Recognition Models","text":"<p>A set of experiments were conducted with a set of medical notes, and a spot-check was conducted to get a general idea of how each NER model performed.</p> <p>Open-source NER Models</p> HuggingFaceSpacy and SciSpacyUniversalNERSpanMarkerNER <p>HuggingFace is a site with a wide range of open-source models that individuals have pre-trained and have made readily availible.</p> <p>Spacy and SciSpacy have a set of pre-trained open-source NER models availible for use. </p> <ul> <li>spacy/en_core_web_md and spacy/en_core_web_lg have been trained OntoNotes 5, Clearnlp Constituient-to-Dependency Conversion, WordNet 3.0, and Explosion vectors, and labels entities such as DATE, EVENT, PERSON, TIME, WORK_OF_ART etc.</li> <li>scispacy/en_core_sci_md have been trained on biomedical data with ~360k vocabulary and 50k word vectors. Same labelling entitiy convention as the spacy/en_core_web_md and spacy/en_core_web_lg models.</li> <li>scispcacy/en_core_sci_scibert have been trained on ~785k vocabulary and allenai/scibert-base as the transformer model. Everything is labelled as ENTITY.</li> </ul> <p>UniversalNER has been trained on data that has been prompted by ChatGPT and has resulted in a dataset that comprises of 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types. </p> <p>UniversalNER works a little different from other NER models, as you have to prompt the model which entity you would like to extract. </p> <p>Therefore this makes the model very good at extracting more diverse entities. </p> <p>There are two ways to prompt the model:</p> <ul> <li>One is hosting UniversalNER locally, and then calling an API to extract the entities from this local server.Example Notebook</li> <li>The other is to quantise the model, and then you can run this model locally.Example Notebook</li> </ul> <p>SpanMarketNER have a large range of pre-trained open source NER models.</p> <ul> <li>span-marker-bert-base-fewnerd-fine-super</li> <li>span-marker-roberta-large-fewnerd-fine-super</li> <li>span-marker-xlm-roberta-base-fewnerd-fine-super</li> <li>span-marker-roberta-large-ontonotes5</li> <li>span-marker-xlm-roberta-large-conll03</li> <li>span-marker-xlm-roberta-large-conll03-doc-context</li> </ul>"},{"location":"open-source-extraction-exploration/named_entity_research/#creating-your-own-entity-labelled-dataset","title":"Creating your own Entity-Labelled Dataset","text":"<p>It is also encouraged in the NER space to label your own data with entities you want to specify and then train a foundation model on this smaller dataset to then label the remaining datasets.</p> <ul> <li>Numind is a powerful foundation model that can be trained on a smaller dataset than previous foundation models (RoBERTa etc.) on a range of datasets. </li> <li>Prodiggy is an annotation tool that has been designed in a way that makes it easy to share the workload of annotating documents, and also verify annotated documents.</li> </ul>"},{"location":"open-source-llm-exploration/","title":"Open source LLM exploration","text":"<p>This section explores the use of open source LLMs for generating unstructured text from structured synthetic data as an extension to the Privacy Fingerprint (PrivFp) Proof of Concept with a focus on running models locally.</p> <p>Experiments and example notebooks for the generative component of PrivFp are available in the privfp-experiments repository.</p>"},{"location":"open-source-llm-exploration/evaluation/","title":"Evaluating generated outputs","text":"<p>How do we run evaluate an open source LLM? The following table contains a (non-exhaustive) list of methods to evaluate an open source LLM.</p> <p>Some of these projects such as lm-evaluation-harness provide extensive evaluation tools however they can be cumbersome to set up and computationally expensive to run locally due to the vast number of requests each evaluation task passes to the LLM.</p> <p>Projects</p> Language Model Evaluation HarnessFastChatAlpacaEvalMeasuring Massive Multitask Language UnderstandingpromptfooagentaPromptToolsOpenAI Evals <p>Framework</p> <p>A unified framework to test LLMs on a large number of different evaluation tasks.</p> <p>GitHub </p> <p>Benchmark, LLM as a judge</p> <ul> <li>Uses MT-bench, a set of challenging multi-turn open-ended questions to evaluate models</li> <li>To automate the evaluation process, FastChat prompts strong LLMs like GPT-4 to act as judges and assess the quality of responses</li> </ul> <p>GitHub </p> <p>Benchmark</p> <ul> <li>An LLM-based automatic evaluation that validated against human annotations</li> <li>Evaluates by measuring the fraction of times a powerful LLM (e.g. GPT-4, Claude or ChatGPT) prefers the outputs from a LLM over outputs from a reference LLM</li> </ul> <p>GitHub  </p> <p>Multiple choice tests</p> <p>A variety of 57 tasks to assess an LLMs general knowledge and ability to problem solve.</p> <p>arXiv </p> <p>Custom</p> <ul> <li>A tool for testing and evaluating LLM output quality</li> <li>Define test cases to score LLM outputs</li> </ul> <p>GitHub </p> <p>Application</p> <ul> <li>An open source LLMOps platform for prompt engineering, evaluation, human feedback, and deployment of complex LLM apps</li> <li>Provides a nice GUI to iterate versions</li> <li>Multiple evaluation methods and metrics available out of the box</li> </ul> <p>GitHub </p> <p>Benchmark, LLM as a judge</p> <ul> <li>Self-host tools for experimenting with, testing, and evaluating LLMs</li> <li>Evaluation tools</li> <li>Supports multiple LLMs, vector databases, frameworks and Stable Diffusion</li> </ul> <p>GitHub </p> <p>Framework</p> <ul> <li>A framework for evaluating LLMs or systems built using LLMs as components</li> <li>Includes an open source registry of challenging evals</li> <li>An \"eval\" is a task used to evaluate the quality of a system's behaviour</li> <li>Requires an OpenAI API key</li> </ul> <p>GitHub </p> <p>When using pre-trained models it may be more effective to review performance benchmarks and metrics already published, for example the chatbot-arena-leaderboard on Hugging Face.</p> <p>If we start fine-tuning models, it may be worth considering a more efficient platform for running evaluation tasks.</p>"},{"location":"open-source-llm-exploration/inference/","title":"Running inference","text":"<p>How do we run inference on an open source LLM? The following table contains a (non-exhaustive) list of methods to interact with an open source LLM.</p> Command linePython libraryApplication <p>Projects</p> llama.cppOllama <p></p> <p>Build and run quantised LLMs via the command line.</p> <p>GitHub </p> <p></p> <p>Serve and run any GGUF format LLM via the Ollama CLI.</p> <p>Ollama</p> <p>Projects</p> HF TransformersLangChain <p>HF Transformers provides APIs and tools to easily run inference on LLMs available from the HF Hub.</p> <p>Hugging Face </p> <p></p> <p>A framework for developing applications powered by LLMs.</p> <p>LangChain</p> <p>Projects</p> HF Text generation inferenceText generation web UILM Studio <p>A toolkit for deploying and serving LLMs.</p> <p>GitHub </p> <p></p> <p>A Gradio web UI for LLMs.</p> <p>GitHub </p> <p>An application to discover, download, and run local LLMs.</p> <p>LM Studio</p> <p>The Awesome-LLM repository also contains a useful list of tools for deploying LLMs.</p> <p>Deploying tools</p>"},{"location":"open-source-llm-exploration/metrics/","title":"Metrics to assess the quality of generated outputs","text":"<p>This section discusses particular quantitative metrics we could use. It would be useful to evaluate on domain specific tasks as well as general LLM tasks.</p>"},{"location":"open-source-llm-exploration/metrics/#topics","title":"Topics","text":"<ul> <li>Coherence: Does the output make sense?</li> <li>Relevance: Is the output relevant to the prompt?</li> <li>Fluency: Is the output grammatically correct?</li> <li>Context understanding: Is the output style correct?</li> <li>Diversity: How much does the output style vary?</li> </ul>"},{"location":"open-source-llm-exploration/metrics/#metrics","title":"Metrics","text":"<p>HF Evaluate Metric</p> <p>Provides a wide range of evaluation metrics out of the box. Here are three examples:</p> <ul> <li> <p>Perplexity</p> <ul> <li>Documentation</li> <li>Perplexity is a measurement of how well a probability distribution or probability model predicts a sample</li> <li>Intuitively, perplexity can be understood as a measure of uncertainty. The perplexity of a language model can be seen as the level of perplexity when predicting the next word in a sequence. Good read: Understanding evaluation metrics for language models</li> <li>Practically, calculation of perplexity will depend on the context length of the LLM and HF Transformers provides an example on how to do this using a \"sliding window\" approach.</li> </ul> </li> <li> <p>BLEU</p> <ul> <li>Bilingual Evaluation Understudy is a metric calculated by comparing machine/human natural language translations</li> <li>Requires human translation reference</li> </ul> </li> <li> <p>ROUGE</p> <ul> <li>Recall-Oriented Understudy for Gisting Evaluation is a metric calculated by comparing machine/human summarisations</li> <li>Requires human summarisation reference</li> </ul> </li> </ul>"},{"location":"open-source-llm-exploration/metrics/#further-reading","title":"Further reading","text":"<ul> <li> <p>A Metrics-First Approach to LLM Evaluation</p> </li> <li> <p>Semantic Uncertainty</p> <p>Introduce semantic entropy, a metric which incorporates linguistic invariances created by shared meanings to provide a more - predictive metric of model accuracy.</p> </li> <li> <p>FEVER</p> <p>Introduces Fact Extraction and VERification, a publicly available dataset for verification against textual sources</p> </li> <li> <p>ProoFVer</p> <p>Proposes fact verification system which uses a seq2seq model to generate natural logic-based inferences as proofs.</p> </li> </ul>"},{"location":"open-source-llm-exploration/metrics/#challenges","title":"Challenges","text":"<ul> <li>Subjective human evaluation</li> <li>Over-reliance on perplexity</li> <li>Difficult to capture diversity and creativity</li> <li>Metric performance won't necessarily translate to real use case performance</li> <li>Dataset bias</li> </ul>"},{"location":"open-source-llm-exploration/search/","title":"Searching for open source LLMs","text":""},{"location":"open-source-llm-exploration/search/#repository-collections","title":"Repository collections","text":"<p>These repositories keep track of some of the most popular LLMs. Awesome-LLM in particular is a great resource for everything LLM.</p> <p>Open LLMs Awesome-LLM</p>"},{"location":"open-source-llm-exploration/search/#hugging-face","title":"Hugging Face","text":"<p>Hugging Face (HF) is a platform containing models, datasets, and demo apps. All HF resources are open source and publicly available unless otherwise stated.</p> <p>Overview Models</p>"},{"location":"open-source-llm-exploration/search/#choosing-a-model","title":"Choosing a model","text":"<p>There is a vast quantity of LLMs readily available but how do we choose the right one?</p> <p>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</p> <p>Models trained specifically for clinical text generation</p> GatorTronGPTMedAlpaca <ul> <li>A study of generative large language model for medical research and healthcare</li> <li>GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records</li> <li>Hugging Face: gatortronS</li> <li>Megatron-LM</li> </ul> <ul> <li>Hugging Face: medalpaca-7b</li> </ul>"},{"location":"open-source-llm-exploration/setup/","title":"Setting up open source LLMs","text":"<p>It is possible to run LLMs locally on consumer CPUs or GPUs with usable performance and inference speed. The following table contains a (non-exhaustive) list of projects which document the process of setting up a local open source LLM.</p> <p>Projects</p> llama.cppOllamaPrivateGPTGPT4ALLTheBlokeExLlamaV2 <p></p> <p>Note: GGML superseded by GGUF</p> <ul> <li>Convert LLMs to GGML format and quantise</li> <li>Manifesto</li> <li>Supports builds for many other LLMs</li> <li>Building from source will require additional software e.g. Make, CMake, Zig or GMake.</li> </ul> <p>Important reading:</p> <ul> <li>GGML format</li> <li>GGML project</li> </ul> <p>Further reading:</p> <ul> <li>How llama.cpp is possible</li> <li>Python bindings for the Transformer models implemented in C/C++ using GGML library</li> </ul> <p>GitHub </p> <p></p> <ul> <li>Download and run pre-built LLMs locally from the ollama.ai library</li> <li>Supports LLMs not in their library by importing GGUF files</li> </ul> <p>GitHub </p> <p>Production-ready AI project that allows you to ask questions about your documents.</p> <p>GitHub </p> <p>LLMs that run locally on your CPU and nearly any GPU.</p> <p>GitHub </p> <p>Pre-built quantised LLMs on HuggingFace.</p> <p>Hugging Face </p> <p>Inference library for running local LLMs on modern consumer GPUs.</p> <p>GitHub </p>"},{"location":"open-source-llm-exploration/setup/#quantisation","title":"Quantisation","text":"<p>The ability to run LLMs on consumer grade hardware has been achieved by quantisation or \"rounding\" of floating point data types. This is accomplished by mapping floating point ranges into more compact integer representations for example, quantising the range (-1.0, ..., 1.0) to (-127, -126, ..., 126, 127). The following links provide a nice introduction to floating point data types and quantisation techniques.</p> <ul> <li>Introduction to 8-bit quantisation</li> <li>4-bit quantisation</li> </ul> <p></p> <p>Good reads</p> <p>Introduction to open source LLMs.</p> <ul> <li>The History of Open-Source LLMs - Early Days (Part One)</li> <li>The History of Open-Source LLMs - Better Base Models (Part Two)</li> </ul>"},{"location":"pycanon/pycanon_and_privacy_metrics/","title":"PyCanon and Privacy Metrics","text":"<p>PyCanon is a python library which asses the values of common privacy measuring metrics, such as k-Anonymity, i-Diversity and t-Closeness. This page will explain the metrics used in PyCanon, alongside other metrics we have researched and considered.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#pycanon-metrics","title":"PyCanon Metrics","text":"<p>To understand the definitions below, we must first understand identifiers, quasi-identifiers and sensitive attributes. There are defined in the PyCanon paper.</p> <p>Identifier: This is any piece of information, such as a full name or NHS number, which can directly identify an individual.</p> <p>Quasi-Identifier This is a piece of information that when combined with other Quasi-Identifiers can make identification possible. This could include birthdays and education information.</p> <p>Sensitive Attributes: Sensitive attributes contain private information that must not be extracted.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#k-anonymity","title":"k-Anonymity","text":"<p>As defined in the PyCanon paper, a database verifies k-anonymity if for each row of the database, there are at least <code>k-1</code> indistinguishable rows with respect to the quasi-identifiers. The value of <code>k</code> deemed acceptable has to be decided beforehand. The probability of identifying an individual in the database using the quasi-identifiers will be at most <code>1/k</code>.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#k-anonymity_1","title":"(\u03b1,k)-Anonymity","text":"<p>Given only one sensitive attribute <code>S</code>, it is checked if the database is k-anonymous and the frequency of each possible value of <code>S</code> is lower or equal than \u03b1 in each equivalence class.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#l-diversity","title":"l-Diversity","text":"<p>k-anonymity gets you equivalence classes - sets of records in the dataset which share the same values for their quasi identifiers. l-diversity is an extension of this. In the case of one sensitive attribute, it checks whether there are at least <code>l</code> distinct values for <code>S</code>. <code>l</code> must always be greater than or equal to 1.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#entropy-l-diversity","title":"Entropy l-Diversity","text":"<p>A database verifies this condition if the entropy of each equivalence class is greater than <code>log(l)</code>. The entropy is defined as:</p> <p>$$ H(EC) = - \\sum_{s \\in D} p(EC,S) \\log_2(p(EC,S)) $$</p> <p>where <code>H</code> is the entropy, <code>EC</code> is the equivalence class, <code>D</code> is the domain of <code>S</code>, where <code>S</code> is a sensitive attribute, and <code>p((EC,S))</code> is the fraction of of records in the equivalence class that have <code>S</code> as a sensitive attribute.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#recursive-cl-diversity","title":"Recursive (c,l)-Diversity","text":"<p>If a sensitive value is removed from an equivalence class that verifies (c,l) diversity, then (c,l-1) diversity is preserved. PyCanon uses the technique defined in this paper. If there are <code>n</code> different values of of a sensitive attribute in an equivalence class, (c,l)-diversity is verified by looking at the the number of times the i-th most frequent values appear in the equivalence class.</p> <p>For a more detailed explanation see the official paper.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#basic-likeness-and-enhanced-likeness","title":"Basic \u03b2-Likeness and Enhanced \u03b2-Likeness","text":"<p>These two techniques can be used in order to control the distance between the distribution of a sensitive attribute in an equivalence class and in the entire database. They follow the method implimented in this paper.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#t-closeness","title":"t-Closeness","text":"<p>A database with one sensitive attribute <code>S</code> verifies t-closeness if all the equivalence classes verify it. An equivalence class verifies t-closeness if the distribution of the values of <code>S</code> are at a distance no closer than <code>t</code> from the distribution of the sensitive attribute in the whole database.</p> <p>In PyCanon, Earth Mover's distance is used. See this paper and the PyCanon paper for more information.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#-disclosure-privacy","title":"\u03b4-Disclosure Privacy","text":"<p>For a dataset with only one sensitive attribute <code>S</code>, the \u03b4-disclosure privacy is verified if:</p> <p>$$ \\log\\left(\\frac{p(EC,S)}{p(D,S)}\\right) &lt; \\delta_s, $$</p> <p>for every sensitive attribute in the dataset, and every equivalence class. <code>p(EC,S)</code> is the fraction of records with <code>S</code> as a sensitive attribute in the equivalence class <code>EC</code>, and <code>D</code> is the dataset.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#other-metrics","title":"Other Metrics","text":"<p>There are various other methods for measuring privacy which are not included in the PyCanon tool. They are well explained in these blogs. We will discuss K-Map alongside \u03b4-presence. </p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#k-map","title":"k-Map","text":"<p>Like k-Anonymity, k-Map requires a list of quasi-identifiers. It also requires a larger re-identification dataset. This could be an original dataset before anonymisation, or another larger dataset of everyone living in the UK, for example a census. Your data satisfies k-map if every combination of values for the quasi identifiers appears at least <code>k</code> times in the re-identification dataset. Calculating k-Map is very computationally expensive.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#-presence","title":"\u03b4-Presence","text":"<p>Similar to k-map, \u03b4-presence is a metric that quantifies the probability that an individual belongs to an anonymised dataset. It is also very computationally expensive.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#visualisations","title":"Visualisations","text":"<p>Google Cloud's Looker Studio has a visualisation suite for privacy metrics.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#anomoly-detection","title":"Anomoly Detection","text":"<p>Our current privacy scoring method uses Named-Entity-Recognition (NER) to extract identifiers and quasi-identifiers, then PyCorrectMatch measures the data uniqueness of each piece of text based on the extracted entities. Each row is given a score on how unique the data is. Unique pieces of data are assumed to have a higher risk of re-identification.</p> <p>Following this idea, we instead can explore the use of anomoly detection to identify high risk columns.</p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#distance-based-methods","title":"Distance Based Methods","text":"<p>Distance metrics can be used to find outliers. The greater the distance away from other individuals, the more unique they are. Possible distance metrics that can be used are:</p> <ul> <li>Euclidean Distance</li> <li>Manhatten Distance</li> <li>Minowski Distance</li> </ul> <p>Different distance metrics will have an effect on which data points are considered outliers. As our data is unlikely to be high-dimensional, simple methods such as Euclidean Distance can be both effective and explainable. However, for higher dimensional multi-variate data, more advanced distance metrics could be used, such as Mahalanobis which is effective for high dimensional multi-variate data. </p>"},{"location":"pycanon/pycanon_and_privacy_metrics/#novelty-and-outlier-detection","title":"Novelty and Outlier Detection","text":"<p>SKLearn gives examples of models used for anomoly detection, alongside the following definitions for Novelty and Outlier Detection:</p> <p>outlier detection: The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations.</p> <p>novelty detection: The training data is not polluted by outliers and we are interested in detecting whether a new observation is an outlier. In this context an outlier is also called a novelty.</p> <p>We are more interested in outlier detection. However, many of these methods need the seperation of training and testing data to be effective, of which many users of the Privacy Fingerprint pipeline will not have. Some options we can use are:</p> <ul> <li>The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. The <code>fit_predict</code> function in SKLearn allows for predictions to be returned on the training data.</li> <li>The Elliptic Envelope is an object for detecting outliers in a Gaussian distributed dataset. It also has a <code>fit_predict</code> function in SKLearn which allows for predictions to be returned on the training data.</li> </ul>"},{"location":"pycanon/pycanon_and_privacy_metrics/#llm-as-a-privacy-judge","title":"LLM as a (Privacy) Judge","text":"<p>LLM as a judge is a method in which LLM's are used to evaluate the outputs. Following the structure of the prompt in this huggingface example, we can design a prompt which asses the privacy risk of each string. Issues with this approach include:</p> <ul> <li>A model may predict different scores for the same review if prompted on multiple occasions. This could be mitigated by setting the temperature to 0.</li> <li>It is very computationally expensive.</li> </ul>"},{"location":"pycanon/pycanon_and_privacy_metrics/#conclusions","title":"Conclusions","text":"<p>Traditional privacy metrics, such as k-anonymity and l-diversity are well established. K-anonymity was released in 2002 and thus has been used in various reports. As these metrics are so commonly used in privacy studies, it makes sense to implement k-anonymity, l-diversity and t-closeness into our pipeline. </p> <p>Given we use Synthea to generate our synthetic medical notes dataset, we have a a large re-identification dataset we can use for techniques such as k-map. This may also be worth implementing into our pipeline.</p> <p>Determining a row-by-row privacy risk score like those found by PycorrectMatch can be done using distance-based metrics and outlier detection methods. Currently, we have no plan to implement these into our pipelines. </p>"},{"location":"setup/","title":"Setup","text":"<p>This section will provide details for setting up and running this repository and assumes a working knowledge of the software dependencies in the table below.</p> <ul> <li> <p> Git</p> <p>Required to clone this repository as well as Synthea and Synthea International.</p> <p> Git</p> </li> <li> <p> Python</p> <p>This project is built using Python 3.11 release.</p> <p> Python</p> </li> <li> <p> OpenJDK</p> <p>Required to run Synthea.</p> <p> OpenJDK</p> </li> <li> <p> Julia</p> <p>Required to run the CorrectMatch module with a thin Python wrapper.</p> <p> Julia</p> </li> </ul>"},{"location":"setup/environment-setup/","title":"Environment Set-up","text":""},{"location":"setup/environment-setup/#create-a-suitable-environment","title":"Create a suitable environment","text":"<p>This is the main environment used in the repository.</p> <pre><code>python3.11 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npip install -r requirements_docs.txt\n</code></pre> <p>Note that a separate Python 3.9 environment is required to run the scispacy notebooks:</p> <pre><code>python3.9 -m venv .venv_scispacy\nsource .venv_scispacy/bin/activate\npip install -r requirements_scispacy.txt\n</code></pre>"},{"location":"setup/environment-setup/#pre-commit-installation","title":"Pre-commit Installation","text":"<p>This repo uses <code>pre-commit</code> to ensure <code>black</code> and <code>flake8</code> has been applied. You will need to make sure your virtual environment has been activated.</p> <pre><code>source .venv/bin/activate\npre-commit install\n</code></pre>"},{"location":"setup/extraction/","title":"Install UniversalNER Locally","text":"<p>Warning</p> <p>Requires the quantised <code>NER/UniNER-7B-type</code> model placed in the <code>model</code> folder.</p>"},{"location":"setup/extraction/#downloading-a-quantised-version-of-the-model-from-huggingface","title":"Downloading a Quantised version of the model from HuggingFace","text":"<p>This HuggingFace repository holds a range of quantized UniversalNER Models. Any of these models can be downloaded, but make sure you download a model that your RAM can handle.</p> Download the smallest Quantised UniversalNER model from Huggingface<pre><code>cd privfp-experiments\nsource .venv/bin/activate\nhuggingface-cli download yuuko-eth/UniNER-7B-all-GGUF UniversalNER-7B-all-Q4_0.gguf --local-dir ./models\n</code></pre> <p>Warning</p> <p>You may need to change the universal_ner_path set in the ./src/config.py file.</p>"},{"location":"setup/extraction/#quantising-the-universalner-model-yourself","title":"Quantising the UniversalNER model yourself","text":"<p>The quantised model was created by cloning the llama.cpp repository and quantising the <code>Universal-NER/UniNER-7B-type</code> locally to <code>q4_1.gguf</code> format.</p> <p>First you need to install the llama.cpp repo. You need to run the MAKE command to create essential directories. Installing the Llama.cpp repo<pre><code>git clone llama.cpp\ncd llama.cpp\nMAKE # If you got CPU \nMAKE CUBLAS=1 # If you got GPU\n</code></pre></p> <p>In the llamma.cpp repo you will need to create a python environment to install all the necessary requirements Creating a Python Environment for the Llama Repository<pre><code>cd llama.cpp\npython3.9 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre></p> <p>Next you need to download the UniversalNER model. Download UniversalNER from HuggingFace<pre><code>pip install transformers datasets sentencepiece\nhuggingface-cli download Universal-NER/UniNER-7B-type --local-dir models\n</code></pre></p> <p>Next you will convert the model into a 32 floating point size. Convert UniversalNER model to f32 accuracy<pre><code>python convert.py ./models\n</code></pre></p> <p>Then you convert the f32 point to a q4_1 bit of accuracy. Converting UniversalNER f32 to q4_1 accuracy<pre><code>./quantize models/ggml-model-f32.gguf models/quantized_q4_1.gguf q4_1\n</code></pre></p> <p>This quantized model is located in the quantize models folders. This model can then be transderred to our repo's ./model folder.</p> <p>The steps provided above have been retrieved from a medium article </p>"},{"location":"setup/homebrew-install/","title":"HomeBrew Installation","text":"<p>This codebase supports both UNIX (Apple users) and Windows users who have installed Windows Subsystem for Linux (WSL).</p> <p>HomeBrew is the recommended installer used in this repository and will need to be installed. For the official installation guidance click here.</p> <p>If you are using a Windows machine you will need to install Windows Subsystem for Linux (WSL) to use HomeBrew. This is a more complicated process and would require you to:</p> <ol> <li>Install WSL. For guidance click here.</li> <li>Set-up a Sudo User Account.</li> <li>Grant Sudo Permissions to the User Account.</li> <li>Sign-in to your new Sudo User Account to install HomeBrew. (This is because HomeBrew cannot be installed on the root sudo user.)</li> </ol> <p>For more detailed guidance, we have provided more detail below.</p> Recommended Homebrew Set-up for WSL users WSL Users Install Windows Subsystem for Linux (WSL) <p>If you haven't already done so, install WSL by following the official documentation provided by Microsoft: Windows Subsystem for Linux Installation Guide.</p> <pre><code>wsl --install</code></pre> Set up a Sudo User Account <ul> <li>Launch your WSL distribution and open a terminal window.</li> <li>Run the following command and replace username with your desired username, to create a user.</li> </ul> <pre><code>sudo adduser &lt;username&gt;</code></pre> <p>Follow the prompts to set a password and fill in any additional information as required.</p> Grant Sudo Permissions to the User Account <ul> <li>Run the following command and replace username with your desired username, to add your user account to the sudo group.</li> </ul> <pre><code>sudo usermod -aG sudo &lt;username&gt;</code></pre> Switch to Your New User Account <ul> <li>Log out of the current session by typing logout and press Enter.</li> </ul> <pre><code>logout</code></pre> <ul> <li>Log back in using the newly created user account credentials.</li> </ul> <pre><code>su &lt;username&gt;</code></pre> Install Homebrew <ul> <li>With your WSL terminal open and logged in as your newly created user, run the following command to install Homebrew:</li> </ul> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"</code></pre> <p>Then follow the Next Steps given to you when you install HomeBrew to add Homebrew to your PATH.</p> Verify HomeBrew Installation <ul> <li>Close and reopen your WSL terminal to apply any changes to the shell configuration.</li> <li>Run the following command to verify that Homebrew is installed and configured correctly:</li> </ul> <pre><code>brew --version\nbrew update</code></pre> Recommend HomeBrew set-up for Apple Users 1. Install Homebrew <ul> <li>With your WSL terminal open and logged in as your newly created user, run the following command to install Homebrew:</li> </ul> <pre><code>/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n</code></pre> <ul> <li>Then follow the Next Steps given to you when you install HomeBrew to add Homebrew to your PATH.</li> </ul> 2. Verify HomeBrew Installation: <ul> <li>Close and reopen your WSL terminal to apply any changes to the shell configuration.</li> <li>Run the following command to verify that Homebrew is installed and configured correctly:</li> </ul> <pre><code>brew --version\nbrew update\n</code></pre>"},{"location":"setup/julia-install/","title":"Installing Julia","text":"<p>Warning</p> <p>Requires Julia to run. This particular setup was tested with Julia version 1.8.5 on an M1 MBP.</p> <p>CorrectMatch is used via a thin Python wrapper for the Julia module CorrectMatch.jl to estimate uniqueness from small population samples.</p>"},{"location":"setup/julia-install/#install-julia","title":"Install Julia","text":"<p>Open a new terminal and install the recommended julia installer Install Julia<pre><code>curl -fsSL https://install.julialang.org | sh\n</code></pre></p> Download Julia version 1.8.5 and set default to 1.8.5<pre><code>juliaup add 1.8.5\njuliaup default 1.8.5\n</code></pre>"},{"location":"setup/julia-install/#adding-julia-to-path","title":"Adding Julia to Path","text":"<p>You will want to use a text editor such as nano to edit your shell configuration file.</p> Opening shell configuration file for Mac<pre><code>nano ~/.bash_profile\n</code></pre> Opening shell configuration file for WSL<pre><code>nano ~/.bashrc\n</code></pre> <p>Then on this file you will need to add this line to the bottom of this file Adding Julia to path<pre><code>export PATH=\"$HOME/.julia/juliaup/bin:$PATH\"\n</code></pre></p> <p>You will need to reload the terminal OR you can run the following commands:</p> Re-initialising the Shell for Mac<pre><code>source ~/.bash_profile\n</code></pre> Re-initialising the Shell for WSL<pre><code>source ~/.bashrc\n</code></pre>"},{"location":"setup/julia-install/#adding-correctmatch","title":"Adding CorrectMatch","text":"Add CorrectMatch package to Julia<pre><code>julia -e 'using Pkg; Pkg.add(\"CorrectMatch\")'\n</code></pre>"},{"location":"setup/llm/","title":"Install Ollama and set-up Large Language Models","text":"<p>Warning</p> <p>Requires Ollama to run. This particular setup was tested with Ollama version 0.1.27.</p> <p>Ollama is used for the unstructured generative component of Privacy Fingerprint. It provides a simple interface to download quantised models and run inference locally.</p>"},{"location":"setup/llm/#ollama-installation","title":"Ollama Installation","text":"<p>Install Ollama using the curl command. <pre><code>curl https://ollama.ai/install.sh | sh\n</code></pre></p>"},{"location":"setup/llm/#start-ollama","title":"Start Ollama","text":"<p>Either open up the desktop application or a terminal and enter <code>ollama serve</code>.</p>"},{"location":"setup/llm/#ollama-models","title":"Ollama models","text":"<p>To download a model open a terminal and enter <code>ollama pull &lt;model_name&gt;</code>. The example notebooks in this repository currently use <code>llama2:latest fe938a131f40</code>.</p> <p>See the Ollama model library for all available models.</p>"},{"location":"setup/llm/#other-models","title":"Other models","text":"<p>It is possible to use your own models not specified in the Ollama model library. Ollama supports the <code>.gguf</code> format and many quantised and non-quantised models can be found on the Hugging Face Hub.</p> <p>To quantise a model, check out the resources on setting up open source LLMs with llama.cpp and the introductory reading around quantisation.</p>"},{"location":"setup/synthea/","title":"Setting up Synthea International","text":"<p>Synthea is used for the structured generative component of Privacy Fingerprint with UK adaptation from Synthea International.</p>"},{"location":"setup/synthea/#install-openjdk-via-homebrew","title":"Install openjdk via HomeBrew","text":"<p>Warning</p> <p>This particular setup was tested with Homebrew openjdk@17.</p> <p>You will need to ensure HomeBrew is set-up correctly, then you can run: Install openjdk@17<pre><code>sudo apt-get update\nbrew install openjdk@17\n</code></pre></p> <p>You can then verify OpenJDK 17 is installed by checking the version <pre><code>java -version\n</code></pre></p>"},{"location":"setup/synthea/#clone-repositories","title":"Clone repositories","text":"<p>Warning</p> <p>If you are using WSL, ensure you git clone Synthea using WSL. If you want to work in your local C: drive you can access this drive via the /mnt/c folder in your wsl terminal. For more information click here.</p> Open a terminal in a projects directory<pre><code>git clone https://github.com/nhsengland/privfp-experiments.git\ngit clone https://github.com/synthetichealth/synthea.git\ngit clone https://github.com/synthetichealth/synthea-international.git\n</code></pre>"},{"location":"setup/synthea/#modify-synthea","title":"Modify Synthea","text":"Checkout compatible versions<pre><code>cd synthea\ngit checkout fb43d3957762465211e70daeeb2bc00e0dbd1c1d\n\ncd ../synthea-international\ngit checkout b4cd9a30f106957ae8f2682090d45b04a49a7c4b\n</code></pre> Copy UK adaptation<pre><code>cp -R gb/* ../synthea\ncd ../synthea\n</code></pre>"},{"location":"setup/synthea/#verify-installation","title":"Verify installation","text":"Run Synthea<pre><code>./run_synthea \"West Yorkshire\"\n</code></pre>"},{"location":"setup/synthea/#modify-synthea-properties","title":"Modify Synthea Properties","text":"<p>You will need to replace the file inside the synthea repository (synthea/src/main/resources/synthea.properties) with the file located locally in this repository (privfp-experiments/src/generate/synthea_properties/synthea.properties).</p>"},{"location":"shap-explainer/","title":"SHAP (Explainer Module)","text":"<p>This page provides a short summary of SHAP (SHapley Additive exPlanations) and how it may be used to interpret and explain the results produces from CorrectMatch, which evaluates the uniqueness of data rows across a whole dataframe.</p>"},{"location":"shap-explainer/additional-reads/","title":"Additional Resources","text":"<ul> <li>Towards Data Science:\u00a0One Feature Attribution Method to (Supposedly) Rule Them All: Shapley Values</li> <li>Kaggle:\u00a0SHAP Values</li> </ul>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/","title":"How does SHAP Work and can be Applied?","text":"<p>SHAP uses classic Shapley values from a branch of mathematics called game theory and their related extensions.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#cooperative-game","title":"Cooperative game","text":"<p>Imagine a scenario where a group of players collaborates to achieve a certain outcome, and the value of that outcome is generated by the entire group. In the context of a machine learning model, a prediction would be the value and the features would be the players.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#marginal-contribution","title":"Marginal contribution","text":"<p>The marginal contribution of a feature is the change it makes to a prediction when introducing it to the group. However, there is an issue because the order in which a feature is introduced will influence its marginal contribution.</p> <p>For example, consider a simple scenario where we have three players A, B and C. If we want to work out the marginal contribution of each player to the group, it may differ based on the order that each player is introduced to the group. Imagine if players A and B had overlapping skillsets. Then, whichever player A or B was introduced first would provide a larger marginal contribution because part of the second player's contribution would already be included by the first. In other words we need to consider the sequences: ABC, ACB, BAC, BCA, CAB, CBA.</p> <p>This is where we introduce Shapley values.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#shapley-values","title":"Shapley values","text":"<p>Shapley values are computed from averaging the marginal contributions of each feature across all possible permutations of features in the group. In other words, the marginal contribution of each feature is calculated for every possible order that each feature can be introduced to the group and the Shapley value is defined as the average of those marginal contributions for each feature.</p> <p>The sum of the Shapley values for an outcome is equal to the difference between the prediction and some baseline prediction.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#computation","title":"Computation","text":"<p>Notice that in the sequence above, if we were to compute the marginal contribution for player A being introduced in position 0 then we would only need to calculate one of ABC or ACB. This is because of the additive property of Shapley values. The outcome of all the possible sequences should be the same so it doesn't matter what order the rest of the players are introduced after (and before) the player of interest. This simplifies the computation as we can now consider the subsets of sequences in which each player is introduced at a certain position, weight them accordingly and take the average for the Shapley value.</p> <p>With this intuition, the full mathematical expression can be understood and is described in the original paper linked above.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#shap-values","title":"SHAP values","text":"<p>A problem becomes evident when more features are added to a model. The Shapley values become exponentially more expensive to compute and becomes unfeasible very quickly. SHAP proposes using other additive feature attribution methods to approximate calculations such as Local Interpretable Model-agnostic Explanations (LIME) and Deep Learning Important FeaTures (DeepLIFT). For full details of these implementations, refer to the original paper linked above.</p>"},{"location":"shap-explainer/what-is-shap-and-why-use-it/","title":"What is SHAP and Why use it?","text":""},{"location":"shap-explainer/what-is-shap-and-why-use-it/#what-is-shap","title":"What is SHAP?","text":"<p>SHAP is proposed as a unified measure of feature importance of a machine learning model by assigning each feature an importance value for each prediction.</p> <p>The sum of SHAP values for all features is equal to an individual prediction minus the prediction for some baseline values. The graph below illustrates an example for whether a team would have a player win the Man of the Match award in a game of football.</p> <p></p> <p>SHAP is an open source Python library which implements the methods described in the original paper.</p> <ul> <li>SHAP: https://github.com/shap/shap</li> <li>A Unified Approach to Interpreting Model Predictions: https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</li> <li>More citations: https://github.com/shap/shap?tab=readme-ov-file#citations</li> </ul>"},{"location":"shap-explainer/what-is-shap-and-why-use-it/#why-use-shap","title":"Why use SHAP?","text":"<p>It is important to be able to interpret the results of machine learning models and by understanding feature contributions to individual predictions, we can gain insight into how models arrive at specific outcomes. This is crucial for building trust in complex models that are often difficult to interpret.</p>"},{"location":"standardisation-research/","title":"Standardisation","text":"<p>The main aim of this standardisation spike was to explore methods for standardizing outputs from extracting information from medical notes.</p> <p></p>"},{"location":"standardisation-research/#pre-processing-text","title":"Pre-processing Text","text":"<p>We would want to consider expanding out acronyms in this space. It would be good to evaluate the effect of these two acronym expanders:</p> <ul> <li>Ab3P: Biomedical Specific acronym expansion tool has been trained on PubMed Abstracts.</li> <li>Spacy AbbrX: Uses a pre-trained spacy models to expand out acronyms.</li> </ul> <p>You could consider spell checkers, but spell checkers can overcompensate and correct words into those that have alternative meanings.</p>"},{"location":"standardisation-research/#document-to-graph","title":"Document to Graph","text":"<p>This covers possible approaches to transform a document into a graph structure.</p>"},{"location":"standardisation-research/#method-1-text-to-ner-and-relation-extraction-to-graph","title":"Method 1: Text to NER and Relation Extraction to Graph","text":"<ol> <li> <p>Define your entity types, these would be defined by working ideally with the annotator tool to identify all possible entities.</p> </li> <li> <p>Extract out entity values associated with entity type using GliNER.</p> </li> <li> <p>Define relationships between entity types. Create an interface which allows users to define relationships between entity types.</p> <ul> <li>Pros: Generalist approach and keeps a human in the loop.</li> <li>Cons:<ul> <li>Assumes one diagnosis links to symptoms/medication proposed in text isn't always true.</li> <li>You might need to break documents down into extracting out known PID data and this is linked to a patient, and everything else is broken down into sentences and connected back.</li> <li>This might even include overlapping sentences to find where one sentence runs into another.</li> </ul> </li> </ul> </li> <li> <p>Use NetworkX to create your graph and PyVis for visualizing the graph structure.</p> </li> </ol>"},{"location":"standardisation-research/#method-2-text-to-graph-space-for-this-is-growing-so-is-one-to-watch","title":"Method 2: Text to Graph - Space for this is growing so is one to watch.","text":"<p>This is a growing space, this is one approach.</p> <ol> <li>Text2Graph<ul> <li>Limitations:<ul> <li>Probabilistic so the graph varies on each iteration.</li> <li>Computationally expensive.</li> <li>Involves resolving relationships between entities and entity types.</li> </ul> </li> </ul> </li> </ol>"},{"location":"standardisation-research/#method-3-text-to-triplet-to-graph-move-away-from-the-current-approach","title":"Method 3: Text to Triplet to Graph - Move away from the current approach.","text":"<p>We would want to evaluate between some approaches such as:</p> <ol> <li> <p>REBEL:</p> <ul> <li>Limitations:<ul> <li>Bert-based so can only transform 512 tokens at a time. (Consider breaking up the sentence.)</li> <li>Probabilistic so the graph varies on each iteration.</li> <li>Computationally expensive.</li> </ul> </li> </ul> </li> <li> <p>OpenIE Standalone Github Repository</p> <ul> <li>Limitations:<ul> <li>Rule-based approach meaning, unless handled for, triplets extracted are related to the context in the text. This could result in more thorough cleaning needed.</li> <li>To counteract the above you could consider using OpenNRE to extract relations out of the triplets, and make a substitution in the middle triplet.</li> </ul> </li> </ul> </li> </ol>"},{"location":"standardisation-research/#additional-post-processing-steps-to-consider","title":"Additional Post-processing Steps to Consider","text":""},{"location":"standardisation-research/#resolving-entity-relations-normalising-edges","title":"Resolving Entity Relations (Normalising Edges)","text":"<p>Graphs can be converted back into triplets so OpenNRE could be used to extract out the relation between triplets.</p> <p>Limitations:</p> <ol> <li>Not all relations could be extracted out via OpenNRE.</li> <li>Different phrases could still generate different relations from the triplets presented.</li> </ol>"},{"location":"standardisation-research/#resolving-entity-values-normalising-values","title":"Resolving Entity Values (Normalising Values)","text":"<ol> <li>Design a Generalised approach: This will be the default normalisation process, so would allow individuals to consider additional entities.<ul> <li>Applying simple NLP techniques (lowercasing, removing punctuation) - Low Code</li> <li>High Code Implementation (Either Or)<ul> <li>Wikification - Wikimapper is likely the easiest package to implement quickly.<ul> <li>Limitation:: If a value is None - it could be assigned a Null value.</li> </ul> </li> <li>String Comparison between entities<ul> <li>Limitations: You would need to determine a cut-off point to determine whether phrases are the same.</li> </ul> </li> <li>Wikification + String Comparison: Use Wikification, and for values assigned None you could consider String Comparison for a high cut-off point.</li> </ul> </li> </ul> </li> <li>Entity type specific: We likely want to handle specific entities in a certain way and have a generalised way to begin with<ul> <li>Person: Explore out extending out the different parts that makes a name.<ul> <li>Probable People: A library for parsing and formatting person names.</li> <li>Python Nameparser: An alternative library for parsing human names.</li> <li>Nominally: Another tool for parsing names.</li> </ul> </li> <li>Date of Birth: Normalise using DateParser<ul> <li>Limitation:<ul> <li>DateParser will provide None if the value doesn't exist.</li> <li>DateParser will set something to 01 if the value doesn't exist in the date causing these values to overweight.</li> </ul> </li> </ul> </li> </ul> </li> <li>NHS Number: Extract this using Regex.</li> <li>Diagnosis: This is an example of a medically related entity. We might want to consider mapping medically related entities to ICD 10/ SNOMED Codes<ul> <li>ICD10cm Augmented</li> <li>SNOMED CT Entity Linking Challenge</li> </ul> </li> </ol>"},{"location":"standardisation-research/standardising_entity_values/","title":"Standardising Entity Types and their Values","text":"<p>This GitHub repository posts a lot of material related to named-entity-recognition including methodologies for disambiguation and linking.</p>"},{"location":"standardisation-research/standardising_entity_values/#preprocessing-entity-values","title":"Preprocessing Entity Values","text":""},{"location":"standardisation-research/standardising_entity_values/#general-nlp-cleaning","title":"General NLP Cleaning","text":"<ul> <li>Lowercasing</li> <li>Removing Punctuation</li> <li>Spell Correction<ul> <li>PySpellChecker: Uses a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word.</li> <li>TextBlob: Spelling correction is based on Peter Norvig\u2019s \u201cHow to Write a Spelling Corrector\u201d as implemented in the pattern library. It is about 70% accurate.</li> <li>autocorrect: Also based on Peter Norvig's work.</li> </ul> </li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#expanding-acronyms","title":"Expanding Acronyms","text":"<ul> <li>Ab3P: Biomedical Specific acronym expansion tool trained on PubMed Abstracts.</li> <li>Spacy AbbrX: Uses pre-trained spacy models to expand acronyms.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#coreference-resolution","title":"Coreference Resolution","text":"<p>Implemented in the Neo4j pipeline, it involves replacing all pronouns with the referenced entity, helping resolve relationships between entities.</p>"},{"location":"standardisation-research/standardising_entity_values/#expanding-specific-entity-types","title":"Expanding Specific Entity Types","text":""},{"location":"standardisation-research/standardising_entity_values/#expanding-names","title":"Expanding Names","text":"<ul> <li>Probable People: Library for parsing and formatting person names.</li> <li>Python Nameparser: An alternative library for parsing human names.</li> <li>Nominally: Another tool for parsing names.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#expanding-location","title":"Expanding Location","text":"<ul> <li>LibPostal: Library for parsing and formatting postal addresses.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#formatting-entity-types","title":"Formatting Entity Types","text":""},{"location":"standardisation-research/standardising_entity_values/#dates","title":"Dates","text":"<ul> <li>DateParser: Parses dates into the same format.</li> <li>Microsoft Recognizers-Text: Open-source and can be used locally. Supports multiple languages and various data types as detailed on their GitHub repo readme.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#phrases-based-on-context","title":"Phrases based on Context","text":"<ul> <li>ExtEnD (Extractive Entity Disambiguation): Can be integrated with the spacy framework to extend meanings of words using surrounding context.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#resolving-entity-values","title":"Resolving Entity Values","text":""},{"location":"standardisation-research/standardising_entity_values/#entity-disambiguation-and-entity-linking","title":"Entity Disambiguation and Entity Linking","text":"<p>Wikification</p> <ul> <li> <p>REL (Radboud Entity Linker): Uses the English Wikipedia as a knowledge source. Maps entities to Wikipedia IDs and normalizes outputs.</p> </li> <li> <p>Wikimapper: Small Python library that maps Wikipedia page titles.</p> </li> <li> <p>spacy_entity_linker: Uses a knowledge base (Wikipedia) to find similar entities.</p> </li> <li> <p>Neo4J (uses Bloom): Resolves entities.</p> </li> <li> <p>BLINK: Facebook's architecture for linking entities to Wikipedia.</p> </li> <li> <p>SpacyFishing: Framework to fetch wiki IDs for extracted entity values.</p> </li> <li> <p>BENT: Open-source repo aimed at resolving entity ambiguity and linking for biomedical terms.</p> </li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#entity-embedding-based-normalisation-approaches","title":"Entity Embedding-based Normalisation Approaches","text":"<ul> <li>Entity Embed: A PyTorch library for embedding entities into vectors to support scalable record linkage and entity resolution.</li> <li>StarSpace: Facebook tool for embedding entities and link prediction in knowledge bases.</li> <li>Spacy Dependency Parser: Extracts nouns from text, uses knowledge bases, and derives roots from texts.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#medical-code-resolution","title":"Medical Code Resolution","text":"<ul> <li>ICD10cm Augmented: John Snow Labs tool for resolving clinical text using ICD-10 codes.</li> <li>Healthcare Relation Extraction: Notebook demonstrating relation extraction in healthcare data using John Snow Labs tools with ICD-10 codes.</li> <li>SNOMED CT Entity Linking Challenge: Competition for linking text spans in clinical notes with specific topics in the SNOMED CT clinical terminology.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#string-comparisons-between-entities","title":"String Comparisons between Entities","text":"<ul> <li>Jellyfish: Library for approximate and phonetic matching of strings.</li> <li>PyStringMatching: Library for string matching.</li> <li>TextDistance: Library for measuring text similarity.</li> <li>StringCompare: Library for comparing string structures.</li> <li>Abydos: Supports phonetic algorithms, string distance metrics, stemmers, and string fingerprints.</li> <li>FuzzyWuzzy: Uses Levenshtein distance to identify close strings.</li> </ul>"},{"location":"standardisation-research/standardising_entity_values/#research-paper-concepts","title":"Research Paper Concepts","text":"<ul> <li>BIOSYN with Synonym Marginalisation: Method and framework to train and identify synonyms in biomedical text.</li> <li>Text Normalization Using Encoder\u2013Decoder Networks Based on the Causal Feature Extractor: Research on text normalization.</li> <li>EL-Chatbot: Paper outlining entity linking using a chatbot, with an associated GitHub repo.</li> </ul>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/","title":"Standardising with Knowledge Graphs","text":""},{"location":"standardisation-research/standardising_with_knowledge_graphs/#from-document-to-knowledge-graph","title":"From Document to Knowledge Graph","text":""},{"location":"standardisation-research/standardising_with_knowledge_graphs/#entity-type-entity-value-relationship-extraction","title":"Entity Type - Entity Value Relationship Extraction","text":"<ol> <li> <p>KG-Completion from the Graph4NLP codebase</p> </li> <li> <p>OpenNRE: This is an open-source repository which is used to infer relations from a given sentence.</p> </li> <li> <p>Zett: This is a zero shot entity relation extraction repo where you give the structure you expect the relation to be in and then it extracts out the connecting values from the text.</p> </li> <li> <p>GliREL: You can define the connections between entities using \"glirel labels\" i.e. you could say diagnosis is \"treated with\" medication.</p> </li> <li> <p>GoLLie: Zero-shot approach to extracting out entities, where you provide some general relations you expect to see and this can extract the relations between entities.</p> </li> <li> <p>Research Paper Concepts:</p> <ul> <li> <p>Generative Type Oriented Named Entity Extraction: A research paper on a generative approach to named entity extraction.</p> </li> <li> <p>Co-attention Network for Joint Entity and Relation Extraction: A research paper on using a co-attention network for joint entity and relation extraction, with provided code.</p> </li> </ul> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#document-to-triplets","title":"Document to Triplets","text":"<ol> <li> <p>Text2Graph: This is a pre-trained model on HuggingFace that has been trained by ChatGPT to identify triplets in text.</p> </li> <li> <p>REBEL: This is a pre-trained model on HuggingFace that extracts triplets out from text. (BERT-based model - you would be limited by 512 tokens.)</p> </li> <li> <p>Joint Entity and Relation Extraction: This is a paper outlining the creation of a medically-related dataset to help fine-tune the REBEL model to be better at extracting out medically-related entities.</p> </li> <li> <p>OpenIE Standalone Github Repository: A repository for OpenIE, a tool that extracts entities and their relationships from text.</p> </li> <li> <p>There is an annotation tool called RTE which uses OpenIE to extract out triplets.</p> <ul> <li> <p>Online Tool</p> </li> <li> <p>Paper</p> </li> </ul> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#triplets-to-graph","title":"Triplets to Graph","text":"<p>Structure:</p> <ol> <li> <p>NetworkX: Python package used to create graph data structures.</p> </li> <li> <p>Graph-tools: Python package that provides a number of features for handling directed/undirected graphs and complex networks.</p> </li> </ol> <p>Visualisations:</p> <ol> <li> <p>GraphViz: Python packages to visualise graphs.</p> </li> <li> <p>PyVis: Python package to visualise graphs.</p> </li> <li> <p>IGraph: Python package to visualise graphs.</p> </li> </ol> <p>Graph Databases:</p> <ol> <li> <p>Neo4J: Community Edition which is free, but commercialised would need to be payed for.</p> </li> <li> <p>JanusGraph: Fully open-source under the Apache 2 license - but it only supports Linux, and data storage requires a cost-based platform.</p> </li> <li> <p>ArangoDB: Community Edition which is free, but commercialised would need to be payed for.</p> </li> <li> <p>OrientDB: Community Edition which is free, but commercialised would need to be payed for.</p> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#entity-resolution-pipelines","title":"Entity Resolution Pipelines","text":""},{"location":"standardisation-research/standardising_with_knowledge_graphs/#neo4j","title":"Neo4j","text":"<ol> <li> <p>Neo4j Entity Resolution Example: A GitHub repository with examples of using Neo4j for entity resolution.</p> </li> <li> <p>Neo4j Whitepaper on Graph Databases: A whitepaper explaining the use of graph databases like Neo4j for various applications, including entity resolution.</p> </li> <li> <p>Neo4j Pipeline: Outlines a process entities can be resolved:</p> <ul> <li>Coreference Resolution: Replacing all pronouns with the referenced entity.</li> <li>NER: Extracting out the named entities from the text provided.</li> <li>Entity Disambiguation and Entity Linking: i.e. you could use Wikipedia ID linking - which tries to resolve words that have similar meaning. (\"Wikification\")</li> <li>Co-Occurrence Graphs: This is inferring relationships between a pair of entities based on their presence within a specified unit of text.</li> <li>Relationship Extraction:<ul> <li>Rule-based extraction: use grammatical dependencies to extract relationships out.</li> <li>Used a trained NLP model to extract relationships between pairs of entities out.</li> </ul> </li> </ul> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#tigergraph-and-zingg","title":"TigerGraph and Zingg","text":"<ol> <li> <p>Entity Resolution with TigerGraph: An article discussing how to use TigerGraph and Zingg for entity resolution.</p> </li> <li> <p>Using a Graph Database for Big Data Entity Resolution: A blog post from TigerGraph on using their graph database for big data entity resolution.</p> </li> <li> <p>Zingg Github Repository: The GitHub repository for Zingg, a tool for entity resolution and matching records.</p> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#pyjedai","title":"PyJedAI:","text":"<ol> <li> <p>PyJedAI CleanCleanER: A tutorial for using PyJedAI for entity matching and clustering.</p> </li> <li> <p>PyJedAI Similarity Joins: A tutorial for using PyJedAI for similarity joins in entity resolution.</p> </li> <li> <p>ER Evaluation Framework: A framework for evaluating entity resolution systems.</p> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#rebel-llama-index","title":"REBEL + Llama Index:","text":"<p>REBEL extracts triplets from text: This is chunked to ensure REBEL can extract the information out.</p>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#knowledgegraph","title":"KnowledgeGraph","text":"<p>KnowledgeGraph: This demonstrates a framework from going from document to graph - the codebase would likely need reworking.</p> <ol> <li> <p>Use Mistral7B OpenOrca hosted by Ollama: For extracting out triplets.</p> </li> <li> <p>NetworkX to make graphs.</p> </li> <li> <p>PyVis to visualise the graphs.</p> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#graph_maker-requires-groq","title":"Graph_Maker: Requires GROQ","text":"<ol> <li> <p>Define your own ontology i.e. your entities and a description of what those entities are.</p> </li> <li> <p>Run the Graph-maker using a large language model to create your graph.</p> </li> <li> <p>Then you can use this graph it has created over your documents.</p> </li> <li> <p>Tutorial</p> </li> </ol>"},{"location":"standardisation-research/standardising_with_knowledge_graphs/#instructor","title":"Instructor:","text":"<ol> <li>Might support Ollama</li> <li>You can follow this tutorial but use the ollama implementation.</li> </ol>"}]}