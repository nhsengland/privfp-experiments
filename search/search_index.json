{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"background/","title":"Background","text":""},{"location":"background/#links","title":"Links","text":"<ul> <li>Original proof of concept repository</li> <li>Experiments repository</li> <li>Unstructured report repository</li> </ul>"},{"location":"background/#related-resources","title":"Related resources","text":"<ul> <li>Exploring Language Modelling for (NHS) Patient Safety Incident Reports</li> </ul>"},{"location":"correct-match/","title":"(Py)Correct Match (Scorer Module)","text":"<p>This page provides a short summary of what (Py)CorrectMatch is and how it may be used to extract out information on the uniqueness of individual records, and the global uniquness across a whole dataset.</p>"},{"location":"correct-match/how-correct-match-works/","title":"How does Correct Match Work?","text":""},{"location":"correct-match/how-correct-match-works/#1extract-the-empirical-marginal-distribution-from-each-feature","title":"1.Extract the Empirical Marginal Distribution from each feature.","text":"<ul> <li>A variety of marginal distributions are fitted to the feature data:<ul> <li>Negative Binomial</li> <li>Geometric</li> <li>Categorical (probability of frequency counts)</li> <li>Logarithmic</li> </ul> </li> <li>Bayesian Information Criterion (BIC) is used to determine the best fitting distirbution.</li> </ul>"},{"location":"correct-match/how-correct-match-works/#2extract-the-correlation-matrix-between-features","title":"2.Extract the Correlation Matrix between features","text":""},{"location":"correct-match/how-correct-match-works/#3fit-a-gaussian-copula-using-the-estimated-marginal-distributions","title":"3.Fit a Gaussian Copula using the estimated Marginal Distributions","text":"<ul> <li>Gaussian Copula is fitted to the dataset using maximum likelihood estimation (MLE).</li> <li>An optimization procedure is performed to fit the correlation matrix of the Gaussian Copula. The optimization aims to minimize the difference between the calculated mutual information and the target mutual information.</li> </ul>"},{"location":"correct-match/how-correct-match-works/#4analogy-to-summarise-how-a-fitted-gaussian-copula-understands-uniqueness-of-individuals-and-across-a-whole-dataset","title":"4.Analogy to summarise how a fitted Gaussian Copula understands uniqueness of individuals and across a whole dataset.","text":"<ul> <li>Gaussian Copula Model: Imagine you have a recipe for cookies that includes various ingredients like flour, sugar, chocolate chips, etc. Each ingredient represents a variable in your dataset, and the recipe captures the correlations between these ingredients (variables).</li> <li>Generate Random Samples: Now, instead of using exact measurements from the recipe, you randomly adjust the quantities of ingredients within reasonable ranges, maintaining the overall proportions specified by the recipe. These randomly adjusted ingredient quantities represent the random samples generated from the Gaussian copula model.</li> <li>Calculate Cell Probabilities: Estimating cell probabilities involves determining the likelihood of an individual data point (or in our analogy, a batch of cookies with specific ingredient quantities) falling into each combination of marginals within the joint distribution modelled by the Gaussian copula. Based on these observations, you estimate the probability of each combination of ingredient quantities producing the desired cookies. </li> </ul> <p>Similarly, in the data context, the smooth_weight function estimates the probability of each combination of discrete values for an individual's data based on the random samples generated from the Gaussian copula model.</p> <p></p>"},{"location":"correct-match/what-is-correct-match/","title":"What is (Py)Correct Match?","text":"<p>Correct Match is a Julia module consisting of several components for analysing data uniqueness using a Gaussian copula model.</p> <p>Top Level Overview: Gaussian copula model is trained across the whole dataframe, then this model is used to assess and extract the \"uniqueness\" score for each individual data point. </p> <p></p>"},{"location":"open-source-extraction-exploration/","title":"Open source Extraction Exploration","text":"<p>Once we have LLM-generated medical notes we then want to extract entities from these notes to then produce a privacy risk score. </p> <p>In previous work, AWS Comprehend Medical was first used to extract entities from these medical notes. In this project we want to explore using open-source named-entity extraction methods that could be used instead of AWS Comprehend Medical.</p> <p>Experiments and example notebooks for the extraction component of PrivFp are available in the privfp-experiments repository.</p>"},{"location":"open-source-extraction-exploration/deidentification_research/","title":"De-Identification Methods","text":"<p>Deidentification techniques refer to methods used to remove or mask personally identifiable information (PII) from data, while still retaining its utility for analysis or other purposes. </p> <p>De-Identification Methods</p> PresidoAnonCATMASKCloud Data Loss Prevention API <p>Presido is a de-identification SDK owned by microsoft.</p> <ul> <li>Regex to recognise patterns.</li> <li>Use Named-entity recognition model (default is set to en_core_web_lg and supports any Spacy model.)</li> <li>Validating patterns</li> <li>Uses context to increase detection confidence.</li> </ul> <p>AnonCAT is a transformer based approach fo redacting text from electronic health records.</p> <p>It uses a NER model, en_core_sci_md, to detect all medical terms. Then they assign each entitity to an ID in a biomedical databases (UMLS) to normalise the outputs. (decipher new diagnosis, history, or reason for admission.)</p> <p>MASK is Manchester University de-identification framework for named-entitity-recognition.</p> <ul> <li>BiLSTM layer - essentially considers each of the entities (beginning, in the middle, no entity) - and assigns a confidence value to help determine it's label across.</li> <li>CRFs Layer use the observed data to predict the labels of the sequence, while taking into account the dependencies between neighbouring labels. (Conditional Random Field)</li> <li>GLoVe embeddings - determines a word vector space that incorporates both the local context of words but also their co-occurence with other words across the space. </li> <li>ELMo embeddings - considers where words have the same spelling but different meaning (Polysemy.)  - takes the word representations and then take the entire input sentence into equation for calculating the word embeddings.</li> </ul> <p>DLP API is googles API for detection of privacy-sensitive fragments in text, images, and Google Cloud Platform storage repositories.</p> <p>There is a range of techniques that are implemented in this API, some noticeable ones being:</p> <ul> <li>Using basic RegexMatching for some PID data. etc. phone numbers.</li> <li>Using a hotword rule to instruct Sensitive Data Protection to adjust the likelihood of a finding, depending on whether a hotword occurs near that finding.</li> <li>Using exclusion rules to exclude false or unwanted finding (identifying custom substrings within a string.) - for example name inside an email.(Take the email, and not the name)</li> </ul>"},{"location":"open-source-extraction-exploration/named_entity_research/","title":"Open-source Named-entity Recognition Models","text":"<p>Named-entity Recognition (NER) models aim to label words/phrases in unstructured data as a classified entity.</p>"},{"location":"open-source-extraction-exploration/named_entity_research/#named-entity-recognition-models","title":"Named-entity Recognition Models","text":"<p>A set of experiments were conducted with a set of medical notes, and a spot-check was conducted to get a general idea of how each NER model performed.</p> <p>Open-source NER Models</p> HuggingFaceSpacy and SciSpacyUniversalNERSpanMarkerNER <p>HuggingFace is a site with a wide range of open-source models that individuals have pre-trained and have made readily availible.</p> <p>Spacy and SciSpacy have a set of pre-trained open-source NER models availible for use. </p> <ul> <li>spacy/en_core_web_md and spacy/en_core_web_lg have been trained OntoNotes 5, Clearnlp Constituient-to-Dependency Conversion, WordNet 3.0, and Explosion vectors, and labels entities such as DATE, EVENT, PERSON, TIME, WORK_OF_ART etc.</li> <li>scispacy/en_core_sci_md have been trained on biomedical data with ~360k vocabulary and 50k word vectors. Same labelling entitiy convention as the spacy/en_core_web_md and spacy/en_core_web_lg models.</li> <li>scispcacy/en_core_sci_scibert have been trained on ~785k vocabulary and allenai/scibert-base as the transformer model. Everything is labelled as ENTITY.</li> </ul> <p>UniversalNER has been trained on data that has been prompted by ChatGPT and has resulted in a dataset that comprises of 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types. </p> <p>UniversalNER works a little different from other NER models, as you have to prompt the model which entity you would like to extract. </p> <p>Therefore this makes the model very good at extracting more diverse entities. </p> <p>There are two ways to prompt the model:</p> <ul> <li>One is hosting UniversalNER locally, and then calling an API to extract the entities from this local server.Example Notebook</li> <li>The other is to quantise the model, and then you can run this model locally.Example Notebook</li> </ul> <p>SpanMarketNER have a large range of pre-trained open source NER models.</p> <ul> <li>span-marker-bert-base-fewnerd-fine-super</li> <li>span-marker-roberta-large-fewnerd-fine-super</li> <li>span-marker-xlm-roberta-base-fewnerd-fine-super</li> <li>span-marker-roberta-large-ontonotes5</li> <li>span-marker-xlm-roberta-large-conll03</li> <li>span-marker-xlm-roberta-large-conll03-doc-context</li> </ul>"},{"location":"open-source-extraction-exploration/named_entity_research/#creating-your-own-entity-labelled-dataset","title":"Creating your own Entity-Labelled Dataset","text":"<p>It is also encouraged in the NER space to label your own data with entities you want to specify and then train a foundation model on this smaller dataset to then label the remaining datasets.</p> <ul> <li>Numind is a powerful foundation model that can be trained on a smaller dataset than previous foundation models (RoBERTa etc.) on a range of datasets. </li> <li>Prodiggy is an annotation tool that has been designed in a way that makes it easy to share the workload of annotating documents, and also verify annotated documents.</li> </ul>"},{"location":"open-source-llm-exploration/","title":"Open source LLM exploration","text":"<p>This section explores the use of open source LLMs for generating unstructured text from structured synthetic data as an extension to the Privacy Fingerprint (PrivFp) Proof of Concept with a focus on running models locally.</p> <p>Experiments and example notebooks for the generative component of PrivFp are available in the privfp-experiments repository.</p>"},{"location":"open-source-llm-exploration/evaluation/","title":"Evaluating generated outputs","text":"<p>How do we run evaluate an open source LLM? The following table contains a (non-exhaustive) list of methods to evaluate an open source LLM.</p> <p>Some of these projects such as lm-evaluation-harness provide extensive evaluation tools however they can be cumbersome to set up and computationally expensive to run locally due to the vast number of requests each evaluation task passes to the LLM.</p> <p>Projects</p> Language Model Evaluation HarnessFastChatAlpacaEvalMeasuring Massive Multitask Language UnderstandingpromptfooagentaPromptToolsOpenAI Evals <p>Framework</p> <p>A unified framework to test LLMs on a large number of different evaluation tasks.</p> <p>GitHub </p> <p>Benchmark, LLM as a judge</p> <ul> <li>Uses MT-bench, a set of challenging multi-turn open-ended questions to evaluate models</li> <li>To automate the evaluation process, FastChat prompts strong LLMs like GPT-4 to act as judges and assess the quality of responses</li> </ul> <p>GitHub </p> <p>Benchmark</p> <ul> <li>An LLM-based automatic evaluation that validated against human annotations</li> <li>Evaluates by measuring the fraction of times a powerful LLM (e.g. GPT-4, Claude or ChatGPT) prefers the outputs from a LLM over outputs from a reference LLM</li> </ul> <p>GitHub  </p> <p>Multiple choice tests</p> <p>A variety of 57 tasks to assess an LLMs general knowledge and ability to problem solve.</p> <p>arXiv </p> <p>Custom</p> <ul> <li>A tool for testing and evaluating LLM output quality</li> <li>Define test cases to score LLM outputs</li> </ul> <p>GitHub </p> <p>Application</p> <ul> <li>An open source LLMOps platform for prompt engineering, evaluation, human feedback, and deployment of complex LLM apps</li> <li>Provides a nice GUI to iterate versions</li> <li>Multiple evaluation methods and metrics available out of the box</li> </ul> <p>GitHub </p> <p>Benchmark, LLM as a judge</p> <ul> <li>Self-host tools for experimenting with, testing, and evaluating LLMs</li> <li>Evaluation tools</li> <li>Supports multiple LLMs, vector databases, frameworks and Stable Diffusion</li> </ul> <p>GitHub </p> <p>Framework</p> <ul> <li>A framework for evaluating LLMs or systems built using LLMs as components</li> <li>Includes an open source registry of challenging evals</li> <li>An \"eval\" is a task used to evaluate the quality of a system's behaviour</li> <li>Requires an OpenAI API key</li> </ul> <p>GitHub </p> <p>When using pre-trained models it may be more effective to review performance benchmarks and metrics already published, for example the chatbot-arena-leaderboard on Hugging Face.</p> <p>If we start fine-tuning models, it may be worth considering a more efficient platform for running evaluation tasks.</p>"},{"location":"open-source-llm-exploration/inference/","title":"Running inference","text":"<p>How do we run inference on an open source LLM? The following table contains a (non-exhaustive) list of methods to interact with an open source LLM.</p> Command linePython libraryApplication <p>Projects</p> llama.cppOllama <p></p> <p>Build and run quantised LLMs via the command line.</p> <p>GitHub </p> <p></p> <p>Serve and run any GGUF format LLM via the Ollama CLI.</p> <p>Ollama</p> <p>Projects</p> HF TransformersLangChain <p>HF Transformers provides APIs and tools to easily run inference on LLMs available from the HF Hub.</p> <p>Hugging Face </p> <p></p> <p>A framework for developing applications powered by LLMs.</p> <p>LangChain</p> <p>Projects</p> HF Text generation inferenceText generation web UILM Studio <p>A toolkit for deploying and serving LLMs.</p> <p>GitHub </p> <p></p> <p>A Gradio web UI for LLMs.</p> <p>GitHub </p> <p>An application to discover, download, and run local LLMs.</p> <p>LM Studio</p> <p>The Awesome-LLM repository also contains a useful list of tools for deploying LLMs.</p> <p>Deploying tools</p>"},{"location":"open-source-llm-exploration/metrics/","title":"Metrics to assess the quality of generated outputs","text":"<p>This section discusses particular quantitative metrics we could use. It would be useful to evaluate on domain specific tasks as well as general LLM tasks.</p>"},{"location":"open-source-llm-exploration/metrics/#topics","title":"Topics","text":"<ul> <li>Coherence: Does the output make sense?</li> <li>Relevance: Is the output relevant to the prompt?</li> <li>Fluency: Is the output grammatically correct?</li> <li>Context understanding: Is the output style correct?</li> <li>Diversity: How much does the output style vary?</li> </ul>"},{"location":"open-source-llm-exploration/metrics/#metrics","title":"Metrics","text":"<p>HF Evaluate Metric</p> <p>Provides a wide range of evaluation metrics out of the box. Here are three examples:</p> <ul> <li> <p>Perplexity</p> <ul> <li>Documentation</li> <li>Perplexity is a measurement of how well a probability distribution or probability model predicts a sample</li> <li>Intuitively, perplexity can be understood as a measure of uncertainty. The perplexity of a language model can be seen as the level of perplexity when predicting the next word in a sequence. Good read: Understanding evaluation metrics for language models</li> <li>Practically, calculation of perplexity will depend on the context length of the LLM and HF Transformers provides an example on how to do this using a \"sliding window\" approach.</li> </ul> </li> <li> <p>BLEU</p> <ul> <li>Bilingual Evaluation Understudy is a metric calculated by comparing machine/human natural language translations</li> <li>Requires human translation reference</li> </ul> </li> <li> <p>ROUGE</p> <ul> <li>Recall-Oriented Understudy for Gisting Evaluation is a metric calculated by comparing machine/human summarisations</li> <li>Requires human summarisation reference</li> </ul> </li> </ul>"},{"location":"open-source-llm-exploration/metrics/#further-reading","title":"Further reading","text":"<ul> <li> <p>A Metrics-First Approach to LLM Evaluation</p> </li> <li> <p>Semantic Uncertainty</p> <p>Introduce semantic entropy, a metric which incorporates linguistic invariances created by shared meanings to provide a more - predictive metric of model accuracy.</p> </li> <li> <p>FEVER</p> <p>Introduces Fact Extraction and VERification, a publicly available dataset for verification against textual sources</p> </li> <li> <p>ProoFVer</p> <p>Proposes fact verification system which uses a seq2seq model to generate natural logic-based inferences as proofs.</p> </li> </ul>"},{"location":"open-source-llm-exploration/metrics/#challenges","title":"Challenges","text":"<ul> <li>Subjective human evaluation</li> <li>Over-reliance on perplexity</li> <li>Difficult to capture diversity and creativity</li> <li>Metric performance won't necessarily translate to real use case performance</li> <li>Dataset bias</li> </ul>"},{"location":"open-source-llm-exploration/search/","title":"Searching for open source LLMs","text":""},{"location":"open-source-llm-exploration/search/#repository-collections","title":"Repository collections","text":"<p>These repositories keep track of some of the most popular LLMs. Awesome-LLM in particular is a great resource for everything LLM.</p> <p>Open LLMs Awesome-LLM</p>"},{"location":"open-source-llm-exploration/search/#hugging-face","title":"Hugging Face","text":"<p>Hugging Face (HF) is a platform containing models, datasets, and demo apps. All HF resources are open source and publicly available unless otherwise stated.</p> <p>Overview Models</p>"},{"location":"open-source-llm-exploration/search/#choosing-a-model","title":"Choosing a model","text":"<p>There is a vast quantity of LLMs readily available but how do we choose the right one?</p> <p>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</p> <p>Models trained specifically for clinical text generation</p> GatorTronGPTMedAlpaca <ul> <li>A study of generative large language model for medical research and healthcare</li> <li>GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records</li> <li>Hugging Face: gatortronS</li> <li>Megatron-LM</li> </ul> <ul> <li>Hugging Face: medalpaca-7b</li> </ul>"},{"location":"open-source-llm-exploration/setup/","title":"Setting up open source LLMs","text":"<p>It is possible to run LLMs locally on consumer CPUs or GPUs with usable performance and inference speed. The following table contains a (non-exhaustive) list of projects which document the process of setting up a local open source LLM.</p> <p>Projects</p> llama.cppOllamaPrivateGPTGPT4ALLTheBlokeExLlamaV2 <p></p> <p>Note: GGML superseded by GGUF</p> <ul> <li>Convert LLMs to GGML format and quantise</li> <li>Manifesto</li> <li>Supports builds for many other LLMs</li> <li>Building from source will require additional software e.g. Make, CMake, Zig or GMake.</li> </ul> <p>Important reading:</p> <ul> <li>GGML format</li> <li>GGML project</li> </ul> <p>Further reading:</p> <ul> <li>How llama.cpp is possible</li> <li>Python bindings for the Transformer models implemented in C/C++ using GGML library</li> </ul> <p>GitHub </p> <p></p> <ul> <li>Download and run pre-built LLMs locally from the ollama.ai library</li> <li>Supports LLMs not in their library by importing GGUF files</li> </ul> <p>GitHub </p> <p>Production-ready AI project that allows you to ask questions about your documents.</p> <p>GitHub </p> <p>LLMs that run locally on your CPU and nearly any GPU.</p> <p>GitHub </p> <p>Pre-built quantised LLMs on HuggingFace.</p> <p>Hugging Face </p> <p>Inference library for running local LLMs on modern consumer GPUs.</p> <p>GitHub </p>"},{"location":"open-source-llm-exploration/setup/#quantisation","title":"Quantisation","text":"<p>The ability to run LLMs on consumer grade hardware has been achieved by quantisation or \"rounding\" of floating point data types. This is accomplished by mapping floating point ranges into more compact integer representations for example, quantising the range (-1.0, ..., 1.0) to (-127, -126, ..., 126, 127). The following links provide a nice introduction to floating point data types and quantisation techniques.</p> <ul> <li>Introduction to 8-bit quantisation</li> <li>4-bit quantisation</li> </ul> <p></p> <p>Good reads</p> <p>Introduction to open source LLMs.</p> <ul> <li>The History of Open-Source LLMs - Early Days (Part One)</li> <li>The History of Open-Source LLMs - Better Base Models (Part Two)</li> </ul>"},{"location":"setup/","title":"Setup","text":"<p>This section will provide details for setting up and running this repository and assumes a working knowledge of the software dependencies in the table below.</p> <ul> <li> <p> Git</p> <p>Required to clone this repository as well as Synthea and Synthea International.</p> <p> Git</p> </li> <li> <p> Python</p> <p>This project is built using Python 3.11 release.</p> <p> Python</p> </li> <li> <p> OpenJDK</p> <p>Required to run Synthea.</p> <p> OpenJDK</p> </li> <li> <p> Julia</p> <p>Required to run the CorrectMatch module with a thin Python wrapper.</p> <p> Julia</p> </li> </ul>"},{"location":"setup/correctmatch/","title":"CorrectMatch","text":"<p>Warning</p> <p>Requires Julia to run. This particular setup was tested with Julia version 1.8.5 on an M1 MBP.</p> <p>CorrectMatch is used via a thin Python wrapper for the Julia module CorrectMatch.jl to estimate uniqueness from small population samples.</p>"},{"location":"setup/correctmatch/#install-julia","title":"Install Julia","text":"<p>Open a new terminal and install the recommended julia installer Install Julia<pre><code>curl -fsSL https://install.julialang.org | sh\n</code></pre></p> Download Julia version 1.8.5 and set default to 1.8.5<pre><code>juliaup add 1.8.5\njuliaup default 1.8.5\n</code></pre> Add CorrectMatch package to Julia<pre><code>julia -e 'using Pkg; Pkg.add(\"CorrectMatch\")'\n</code></pre>"},{"location":"setup/correctmatch/#verify-installlation","title":"Verify installlation","text":"Run correctmatch<pre><code>import numpy as np\nimport correctmatch\nimport julia\nfrom julia.api import Julia\nimport os\n\npath_julia = os.popen(\"which julia\").read().strip()\njulia.install(julia=path_julia)\nJulia(compiled_modules=False, runtime=path_julia)\n\narr = np.random.randint(1, 5, size=(1000, 5))\ncorrectmatch.precompile()  # precompile the Julia module\ncorrectmatch.uniqueness(arr)  # true uniqueness for 1,000 records\n</code></pre>"},{"location":"setup/extraction/","title":"Entity extraction","text":"<p>Warning</p> <p>Requires the quantised <code>NER/UniNER-7B-type</code> model placed in the <code>model</code> folder.</p>"},{"location":"setup/extraction/#downloading-a-quantised-version-of-the-model-from-huggingface","title":"Downloading a Quantised version of the model from HuggingFace","text":"<p>This HuggingFace repository holds a range of quantized UniversalNER Models. Any of these models can be downloaded, but make sure you download a model that your RAM can handle.</p> Download the smallest Quantised UniversalNER model from Huggingface<pre><code>cd privfp-experiments\nsource .venv/bin/activate\nhuggingface-cli download yuuko-eth/UniNER-7B-all-GGUF UniversalNER-7B-all-Q4_0.gguf --local-dir ./models\n</code></pre> <p>Then you just need to ensure your universal_ner_path is set to the path of the model.</p>"},{"location":"setup/extraction/#quantising-the-universalner-model-yourself","title":"Quantising the UniversalNER model yourself","text":"<p>The quantised model was created by cloning the llama.cpp repository and quantising the <code>Universal-NER/UniNER-7B-type</code> locally to <code>q4_1.gguf</code> format.</p> <p>First you need to install the llama.cpp repo. You need to run the MAKE command to create essential directories. Installing the Llama.cpp repo<pre><code>git clone llama.cpp\ncd llama.cpp\nMAKE # If you got CPU \nMAKE CUBLAS=1 # If you got GPU\n</code></pre></p> <p>In the llamma.cpp repo you will need to create a python environment to install all the necessary requirements Creating a Python Environment for the Llama Repository<pre><code>cd llama.cpp\npython3.9 -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre></p> <p>Next you need to download the UniversalNER model. Download UniversalNER from HuggingFace<pre><code>pip install transformers datasets sentencepiece\nhuggingface-cli download Universal-NER/UniNER-7B-type --local-dir models\n</code></pre></p> <p>Next you will convert the model into a 32 floating point size. Convert UniversalNER model to f32 accuracy<pre><code>python convert.py ./models\n</code></pre></p> <p>Then you convert the f32 point to a q4_1 bit of accuracy. Converting UniversalNER f32 to q4_1 accuracy<pre><code>./quantize models/ggml-model-f32.gguf models/quantized_q4_1.gguf q4_1\n</code></pre></p> <p>This quantized model is located in the quantize models folders. This model can then be transderred to our repo's ./model folder.</p> <p>The steps provided above have been retrieved from a medium article </p>"},{"location":"setup/llm/","title":"Large Language Models","text":"<p>Warning</p> <p>Requires Ollama to run. This particular setup was tested with Ollama version 0.1.27 on an M1 MBP.</p> <p>Ollama is used for the unstructured generative component of Privacy Fingerprint. It provides a simple interface to download quantised models and run inference locally.</p>"},{"location":"setup/llm/#start-ollama","title":"Start Ollama","text":"<p>Either open up the desktop application or a terminal and enter <code>ollama serve</code>.</p>"},{"location":"setup/llm/#ollama-models","title":"Ollama models","text":"<p>To download a model open a terminal and enter <code>ollama pull &lt;model_name&gt;</code>. The example notebooks in this repository currently use <code>llama2:latest fe938a131f40</code>.</p> <p>See the Ollama model library for all available models.</p>"},{"location":"setup/llm/#other-models","title":"Other models","text":"<p>It is possible to use your own models not specified in the Ollama model library. Ollama supports the <code>.gguf</code> format and many quantised and non-quantised models can be found on the Hugging Face Hub.</p> <p>To quantise a model, check out the resources on setting up open source LLMs with llama.cpp and the introductory reading around quantisation.</p>"},{"location":"setup/synthea/","title":"Synthea International","text":"<p>Warning</p> <p>Requires OpenJDK to run. This particular setup was tested with Homebrew openjdk@17 on an M1 MBP.</p> <p>Synthea is used for the structured generative component of Privacy Fingerprint with UK adaptation from Synthea International.</p>"},{"location":"setup/synthea/#clone-repositories","title":"Clone repositories","text":"Open a terminal in a projects directory<pre><code>git clone https://github.com/nhsengland/privfp-experiments.git\ngit clone https://github.com/synthetichealth/synthea.git\ngit clone https://github.com/synthetichealth/synthea-international.git\n</code></pre>"},{"location":"setup/synthea/#modify-synthea","title":"Modify Synthea","text":"Checkout compatible versions<pre><code>cd synthea\ngit checkout fb43d3957762465211e70daeeb2bc00e0dbd1c1d\n\ncd ../synthea-international\ngit checkout b4cd9a30f106957ae8f2682090d45b04a49a7c4b\n</code></pre> Copy UK adaptation<pre><code>cp -R gb/* ../synthea\ncd ../synthea\n</code></pre>"},{"location":"setup/synthea/#verify-installation","title":"Verify installation","text":"Run Synthea<pre><code>./run_synthea \"West Yorkshire\"\n</code></pre>"},{"location":"shap-explainer/","title":"SHAP (Explainer Module)","text":"<p>This page provides a short summary of SHAP (SHapley Additive exPlanations) and how it may be used to interpret and explain the results produces from CorrectMatch, which evaluates the uniqueness of data rows across a whole dataframe.</p>"},{"location":"shap-explainer/additional-reads/","title":"Additional Resources","text":"<ul> <li>Towards Data Science:\u00a0One Feature Attribution Method to (Supposedly) Rule Them All: Shapley Values</li> <li>Kaggle:\u00a0SHAP Values</li> </ul>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/","title":"How does SHAP Work and can be Applied?","text":"<p>SHAP uses classic Shapley values from a branch of mathematics called game theory and their related extensions.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#cooperative-game","title":"Cooperative game","text":"<p>Imagine a scenario where a group of players collaborates to achieve a certain outcome, and the value of that outcome is generated by the entire group. In the context of a machine learning model, a prediction would be the value and the features would be the players.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#marginal-contribution","title":"Marginal contribution","text":"<p>The marginal contribution of a feature is the change it makes to a prediction when introducing it to the group. However, there is an issue because the order in which a feature is introduced will influence its marginal contribution.</p> <p>For example, consider a simple scenario where we have three players A, B and C. If we want to work out the marginal contribution of each player to the group, it may differ based on the order that each player is introduced to the group. Imagine if players A and B had overlapping skillsets. Then, whichever player A or B was introduced first would provide a larger marginal contribution because part of the second player's contribution would already be included by the first. In other words we need to consider the sequences: ABC, ACB, BAC, BCA, CAB, CBA.</p> <p>This is where we introduce Shapley values.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#shapley-values","title":"Shapley values","text":"<p>Shapley values are computed from averaging the marginal contributions of each feature across all possible permutations of features in the group. In other words, the marginal contribution of each feature is calculated for every possible order that each feature can be introduced to the group and the Shapley value is defined as the average of those marginal contributions for each feature.</p> <p>The sum of the Shapley values for an outcome is equal to the difference between the prediction and some baseline prediction.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#computation","title":"Computation","text":"<p>Notice that in the sequence above, if we were to compute the marginal contribution for player A being introduced in position 0 then we would only need to calculate one of ABC or ACB. This is because of the additive property of Shapley values. The outcome of all the possible sequences should be the same so it doesn't matter what order the rest of the players are introduced after (and before) the player of interest. This simplifies the computation as we can now consider the subsets of sequences in which each player is introduced at a certain position, weight them accordingly and take the average for the Shapley value.</p> <p>With this intuition, the full mathematical expression can be understood and is described in the original paper linked above.</p>"},{"location":"shap-explainer/how-does-shap-work-and-be-applied/#shap-values","title":"SHAP values","text":"<p>A problem becomes evident when more features are added to a model. The Shapley values become exponentially more expensive to compute and becomes unfeasible very quickly. SHAP proposes using other additive feature attribution methods to approximate calculations such as Local Interpretable Model-agnostic Explanations (LIME) and Deep Learning Important FeaTures (DeepLIFT). For full details of these implementations, refer to the original paper linked above.</p>"},{"location":"shap-explainer/what-is-shap-and-why-use-it/","title":"What is SHAP and Why use it?","text":""},{"location":"shap-explainer/what-is-shap-and-why-use-it/#what-is-shap","title":"What is SHAP?","text":"<p>SHAP is proposed as a unified measure of feature importance of a machine learning model by assigning each feature an importance value for each prediction.</p> <p>The sum of SHAP values for all features is equal to an individual prediction minus the prediction for some baseline values. The graph below illustrates an example for whether a team would have a player win the Man of the Match award in a game of football.</p> <p></p> <p>SHAP is an open source Python library which implements the methods described in the original paper.</p> <ul> <li>SHAP: https://github.com/shap/shap</li> <li>A Unified Approach to Interpreting Model Predictions: https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html</li> <li>More citations: https://github.com/shap/shap?tab=readme-ov-file#citations</li> </ul>"},{"location":"shap-explainer/what-is-shap-and-why-use-it/#why-use-shap","title":"Why use SHAP?","text":"<p>It is important to be able to interpret the results of machine learning models and by understanding feature contributions to individual predictions, we can gain insight into how models arrive at specific outcomes. This is crucial for building trust in complex models that are often difficult to interpret.</p>"}]}