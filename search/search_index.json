{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"background/","title":"Background","text":""},{"location":"background/#links","title":"Links","text":"<ul> <li>Original proof of concept repository</li> <li>Experiments repository</li> <li>Unstructured report repository</li> </ul>"},{"location":"background/#related-resources","title":"Related resources","text":"<ul> <li>Exploring Language Modelling for (NHS) Patient Safety Incident Reports</li> </ul>"},{"location":"open-source-extraction-exploration/","title":"Open source Extraction Exploration","text":"<p>Once we have LLM-generated medical notes we then want to extract entities from these notes to then produce a privacy risk score. </p> <p>In previous work, AWS Comprehend Medical was first used to extract entities from these medical notes. In this project we want to explore using open-source named-entity extraction methods that could be used instead of AWS Comprehend Medical.</p> <p>Experiments and example notebooks for the extraction component of PrivFp are available in the privfp-experiments repository.</p>"},{"location":"open-source-extraction-exploration/deidentification_research/","title":"De-Identification Methods","text":"<p>Deidentification techniques refer to methods used to remove or mask personally identifiable information (PII) from data, while still retaining its utility for analysis or other purposes. </p> <p>De-Identification Methods</p> PresidoAnonCATMASKCloud Data Loss Prevention API <p>Presido is a de-identification SDK owned by microsoft.</p> <ul> <li>Regex to recognise patterns.</li> <li>Use Named-entity recognition model (default is set to en_core_web_lg and supports any Spacy model.)</li> <li>Validating patterns</li> <li>Uses context to increase detection confidence.</li> </ul> <p>AnonCAT is a transformer based approach fo redacting text from electronic health records.</p> <p>It uses a NER model, en_core_sci_md, to detect all medical terms. Then they assign each entitity to an ID in a biomedical databases (UMLS) to normalise the outputs. (decipher new diagnosis, history, or reason for admission.)</p> <p>MASK is Manchester University de-identification framework for named-entitity-recognition.</p> <ul> <li>BiLSTM layer - essentially considers each of the entities (beginning, in the middle, no entity) - and assigns a confidence value to help determine it's label across.</li> <li>CRFs Layer use the observed data to predict the labels of the sequence, while taking into account the dependencies between neighbouring labels. (Conditional Random Field)</li> <li>GLoVe embeddings - determines a word vector space that incorporates both the local context of words but also their co-occurence with other words across the space. </li> <li>ELMo embeddings - considers where words have the same spelling but different meaning (Polysemy.)  - takes the word representations and then take the entire input sentence into equation for calculating the word embeddings.</li> </ul> <p>DLP API is googles API for detection of privacy-sensitive fragments in text, images, and Google Cloud Platform storage repositories.</p> <p>There is a range of techniques that are implemented in this API, some noticeable ones being:</p> <ul> <li>Using basic RegexMatching for some PID data. etc. phone numbers.</li> <li>Using a hotword rule to instruct Sensitive Data Protection to adjust the likelihood of a finding, depending on whether a hotword occurs near that finding.</li> <li>Using exclusion rules to exclude false or unwanted finding (identifying custom substrings within a string.) - for example name inside an email.(Take the email, and not the name)</li> </ul>"},{"location":"open-source-extraction-exploration/named_entity_research/","title":"Open-source Named-entity Recognition Models","text":"<p>Named-entity Recognition (NER) models aim to label words/phrases in unstructured data as a classified entity.</p>"},{"location":"open-source-extraction-exploration/named_entity_research/#named-entity-recognition-models","title":"Named-entity Recognition Models","text":"<p>A set of experiments were conducted with a set of medical notes, and a spot-check was conducted to get a general idea of how each NER model performed.</p> <p>Open-source NER Models</p> HuggingFaceSpacy and SciSpacyUniversalNERSpanMarkerNER <p>HuggingFace is a site with a wide range of open-source models that individuals have pre-trained and have made readily availible.</p> <p>Spacy and SciSpacy have a set of pre-trained open-source NER models availible for use. </p> <ul> <li>spacy/en_core_web_md and spacy/en_core_web_lg have been trained OntoNotes 5, Clearnlp Constituient-to-Dependency Conversion, WordNet 3.0, and Explosion vectors, and labels entities such as DATE, EVENT, PERSON, TIME, WORK_OF_ART etc.</li> <li>scispacy/en_core_sci_md have been trained on biomedical data with ~360k vocabulary and 50k word vectors. Same labelling entitiy convention as the spacy/en_core_web_md and spacy/en_core_web_lg models.</li> <li>scispcacy/en_core_sci_scibert have been trained on ~785k vocabulary and allenai/scibert-base as the transformer model. Everything is labelled as ENTITY.</li> </ul> <p>UniversalNER has been trained on data that has been prompted by ChatGPT and has resulted in a dataset that comprises of 45,889 input-output pairs, encompassing 240,725 entities and 13,020 distinct entity types. </p> <p>UniversalNER works a little different from other NER models, as you have to prompt the model which entity you would like to extract. </p> <p>Therefore this makes the model very good at extracting more diverse entities. </p> <p>There are two ways to prompt the model:</p> <ul> <li>One is hosting UniversalNER locally, and then calling an API to extract the entities from this local server.Example Notebook</li> <li>The other is to quantise the model, and then you can run this model locally.Example Notebook</li> </ul> <p>SpanMarketNER have a large range of pre-trained open source NER models.</p> <ul> <li>span-marker-bert-base-fewnerd-fine-super</li> <li>span-marker-roberta-large-fewnerd-fine-super</li> <li>span-marker-xlm-roberta-base-fewnerd-fine-super</li> <li>span-marker-roberta-large-ontonotes5</li> <li>span-marker-xlm-roberta-large-conll03</li> <li>span-marker-xlm-roberta-large-conll03-doc-context</li> </ul>"},{"location":"open-source-extraction-exploration/named_entity_research/#creating-your-own-entity-labelled-dataset","title":"Creating your own Entity-Labelled Dataset","text":"<p>It is also encouraged in the NER space to label your own data with entities you want to specify and then train a foundation model on this smaller dataset to then label the remaining datasets.</p> <ul> <li>Numind is a powerful foundation model that can be trained on a smaller dataset than previous foundation models (RoBERTa etc.) on a range of datasets. </li> <li>Prodiggy is an annotation tool that has been designed in a way that makes it easy to share the workload of annotating documents, and also verify annotated documents.</li> </ul>"},{"location":"open-source-llm-exploration/","title":"Open source LLM exploration","text":"<p>This section explores the use of open source LLMs for generating unstructured text from structured synthetic data as an extension to the Privacy Fingerprint (PrivFp) Proof of Concept with a focus on running models locally.</p> <p>Experiments and example notebooks for the generative component of PrivFp are available in the privfp-experiments repository.</p>"},{"location":"open-source-llm-exploration/evaluation/","title":"Evaluating generated outputs","text":"<p>How do we run evaluate an open source LLM? The following table contains a (non-exhaustive) list of methods to evaluate an open source LLM.</p> <p>Some of these projects such as lm-evaluation-harness provide extensive evaluation tools however they can be cumbersome to set up and computationally expensive to run locally due to the vast number of requests each evaluation task passes to the LLM.</p> <p>Projects</p> Language Model Evaluation HarnessFastChatAlpacaEvalMeasuring Massive Multitask Language UnderstandingpromptfooagentaPromptToolsOpenAI Evals <p>Framework</p> <p>A unified framework to test LLMs on a large number of different evaluation tasks.</p> <p>GitHub </p> <p>Benchmark, LLM as a judge</p> <ul> <li>Uses MT-bench, a set of challenging multi-turn open-ended questions to evaluate models</li> <li>To automate the evaluation process, FastChat prompts strong LLMs like GPT-4 to act as judges and assess the quality of responses</li> </ul> <p>GitHub </p> <p>Benchmark</p> <ul> <li>An LLM-based automatic evaluation that validated against human annotations</li> <li>Evaluates by measuring the fraction of times a powerful LLM (e.g. GPT-4, Claude or ChatGPT) prefers the outputs from a LLM over outputs from a reference LLM</li> </ul> <p>GitHub  </p> <p>Multiple choice tests</p> <p>A variety of 57 tasks to assess an LLMs general knowledge and ability to problem solve.</p> <p>arXiv </p> <p>Custom</p> <ul> <li>A tool for testing and evaluating LLM output quality</li> <li>Define test cases to score LLM outputs</li> </ul> <p>GitHub </p> <p>Application</p> <ul> <li>An open source LLMOps platform for prompt engineering, evaluation, human feedback, and deployment of complex LLM apps</li> <li>Provides a nice GUI to iterate versions</li> <li>Multiple evaluation methods and metrics available out of the box</li> </ul> <p>GitHub </p> <p>Benchmark, LLM as a judge</p> <ul> <li>Self-host tools for experimenting with, testing, and evaluating LLMs</li> <li>Evaluation tools</li> <li>Supports multiple LLMs, vector databases, frameworks and Stable Diffusion</li> </ul> <p>GitHub </p> <p>Framework</p> <ul> <li>A framework for evaluating LLMs or systems built using LLMs as components</li> <li>Includes an open source registry of challenging evals</li> <li>An \"eval\" is a task used to evaluate the quality of a system's behaviour</li> <li>Requires an OpenAI API key</li> </ul> <p>GitHub </p> <p>When using pre-trained models it may be more effective to review performance benchmarks and metrics already published, for example the chatbot-arena-leaderboard on Hugging Face.</p> <p>If we start fine-tuning models, it may be worth considering a more efficient platform for running evaluation tasks.</p>"},{"location":"open-source-llm-exploration/inference/","title":"Running inference","text":"<p>How do we run inference on an open source LLM? The following table contains a (non-exhaustive) list of methods to interact with an open source LLM.</p> Command linePython libraryApplication <p>Projects</p> llama.cppOllama <p></p> <p>Build and run quantised LLMs via the command line.</p> <p>GitHub </p> <p></p> <p>Serve and run any GGUF format LLM via the Ollama CLI.</p> <p>Ollama</p> <p>Projects</p> HF TransformersLangChain <p>HF Transformers provides APIs and tools to easily run inference on LLMs available from the HF Hub.</p> <p>Hugging Face </p> <p></p> <p>A framework for developing applications powered by LLMs.</p> <p>LangChain</p> <p>Projects</p> HF Text generation inferenceText generation web UILM Studio <p>A toolkit for deploying and serving LLMs.</p> <p>GitHub </p> <p></p> <p>A Gradio web UI for LLMs.</p> <p>GitHub </p> <p>An application to discover, download, and run local LLMs.</p> <p>LM Studio</p> <p>The Awesome-LLM repository also contains a useful list of tools for deploying LLMs.</p> <p>Deploying tools</p>"},{"location":"open-source-llm-exploration/metrics/","title":"Metrics to assess the quality of generated outputs","text":"<p>This section discusses particular quantitative metrics we could use. It would be useful to evaluate on domain specific tasks as well as general LLM tasks.</p>"},{"location":"open-source-llm-exploration/metrics/#topics","title":"Topics","text":"<ul> <li>Coherence: Does the output make sense?</li> <li>Relevance: Is the output relevant to the prompt?</li> <li>Fluency: Is the output grammatically correct?</li> <li>Context understanding: Is the output style correct?</li> <li>Diversity: How much does the output style vary?</li> </ul>"},{"location":"open-source-llm-exploration/metrics/#metrics","title":"Metrics","text":"<p>HF Evaluate Metric</p> <p>Provides a wide range of evaluation metrics out of the box. Here are three examples:</p> <ul> <li> <p>Perplexity</p> <ul> <li>Documentation</li> <li>Perplexity is a measurement of how well a probability distribution or probability model predicts a sample</li> <li>Intuitively, perplexity can be understood as a measure of uncertainty. The perplexity of a language model can be seen as the level of perplexity when predicting the next word in a sequence. Good read: Understanding evaluation metrics for language models</li> <li>Practically, calculation of perplexity will depend on the context length of the LLM and HF Transformers provides an example on how to do this using a \"sliding window\" approach.</li> </ul> </li> <li> <p>BLEU</p> <ul> <li>Bilingual Evaluation Understudy is a metric calculated by comparing machine/human natural language translations</li> <li>Requires human translation reference</li> </ul> </li> <li> <p>ROUGE</p> <ul> <li>Recall-Oriented Understudy for Gisting Evaluation is a metric calculated by comparing machine/human summarisations</li> <li>Requires human summarisation reference</li> </ul> </li> </ul>"},{"location":"open-source-llm-exploration/metrics/#further-reading","title":"Further reading","text":"<ul> <li> <p>A Metrics-First Approach to LLM Evaluation</p> </li> <li> <p>Semantic Uncertainty</p> <p>Introduce semantic entropy, a metric which incorporates linguistic invariances created by shared meanings to provide a more - predictive metric of model accuracy.</p> </li> <li> <p>FEVER</p> <p>Introduces Fact Extraction and VERification, a publicly available dataset for verification against textual sources</p> </li> <li> <p>ProoFVer</p> <p>Proposes fact verification system which uses a seq2seq model to generate natural logic-based inferences as proofs.</p> </li> </ul>"},{"location":"open-source-llm-exploration/metrics/#challenges","title":"Challenges","text":"<ul> <li>Subjective human evaluation</li> <li>Over-reliance on perplexity</li> <li>Difficult to capture diversity and creativity</li> <li>Metric performance won't necessarily translate to real use case performance</li> <li>Dataset bias</li> </ul>"},{"location":"open-source-llm-exploration/search/","title":"Searching for open source LLMs","text":""},{"location":"open-source-llm-exploration/search/#repository-collections","title":"Repository collections","text":"<p>These repositories keep track of some of the most popular LLMs. Awesome-LLM in particular is a great resource for everything LLM.</p> <p>Open LLMs Awesome-LLM</p>"},{"location":"open-source-llm-exploration/search/#hugging-face","title":"Hugging Face","text":"<p>Hugging Face (HF) is a platform containing models, datasets, and demo apps. All HF resources are open source and publicly available unless otherwise stated.</p> <p>Overview Models</p>"},{"location":"open-source-llm-exploration/search/#choosing-a-model","title":"Choosing a model","text":"<p>There is a vast quantity of LLMs readily available but how do we choose the right one?</p> <p>Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond</p> <p>Models trained specifically for clinical text generation</p> GatorTronGPTMedAlpaca <ul> <li>A study of generative large language model for medical research and healthcare</li> <li>GatorTron: A Large Clinical Language Model to Unlock Patient Information from Unstructured Electronic Health Records</li> <li>Hugging Face: gatortronS</li> <li>Megatron-LM</li> </ul> <ul> <li>Hugging Face: medalpaca-7b</li> </ul>"},{"location":"open-source-llm-exploration/setup/","title":"Setting up open source LLMs","text":"<p>It is possible to run LLMs locally on consumer CPUs or GPUs with usable performance and inference speed. The following table contains a (non-exhaustive) list of projects which document the process of setting up a local open source LLM.</p> <p>Projects</p> llama.cppOllamaPrivateGPTGPT4ALLTheBlokeExLlamaV2 <p></p> <p>Note: GGML superseded by GGUF</p> <ul> <li>Convert LLMs to GGML format and quantise</li> <li>Manifesto</li> <li>Supports builds for many other LLMs</li> <li>Building from source will require additional software e.g. Make, CMake, Zig or GMake.</li> </ul> <p>Important reading:</p> <ul> <li>GGML format</li> <li>GGML project</li> </ul> <p>Further reading:</p> <ul> <li>How llama.cpp is possible</li> <li>Python bindings for the Transformer models implemented in C/C++ using GGML library</li> </ul> <p>GitHub </p> <p></p> <ul> <li>Download and run pre-built LLMs locally from the ollama.ai library</li> <li>Supports LLMs not in their library by importing GGUF files</li> </ul> <p>GitHub </p> <p>Production-ready AI project that allows you to ask questions about your documents.</p> <p>GitHub </p> <p>LLMs that run locally on your CPU and nearly any GPU.</p> <p>GitHub </p> <p>Pre-built quantised LLMs on HuggingFace.</p> <p>Hugging Face </p> <p>Inference library for running local LLMs on modern consumer GPUs.</p> <p>GitHub </p>"},{"location":"open-source-llm-exploration/setup/#quantisation","title":"Quantisation","text":"<p>The ability to run LLMs on consumer grade hardware has been achieved by quantisation or \"rounding\" of floating point data types. This is accomplished by mapping floating point ranges into more compact integer representations for example, quantising the range (-1.0, ..., 1.0) to (-127, -126, ..., 126, 127). The following links provide a nice introduction to floating point data types and quantisation techniques.</p> <ul> <li>Introduction to 8-bit quantisation</li> <li>4-bit quantisation</li> </ul> <p></p> <p>Good reads</p> <p>Introduction to open source LLMs.</p> <ul> <li>The History of Open-Source LLMs - Early Days (Part One)</li> <li>The History of Open-Source LLMs - Better Base Models (Part Two)</li> </ul>"},{"location":"setup/","title":"Setup","text":"<p>This section will provide details for setting up and running this repository and assumes a working knowledge of the software dependencies in the table below.</p> <ul> <li> <p> Git</p> <p>Required to clone this repository as well as Synthea and Synthea International.</p> <p> Git</p> </li> <li> <p> Python</p> <p>This project is built using Python 3.11 release.</p> <p> Python</p> </li> <li> <p> OpenJDK</p> <p>Required to run Synthea.</p> <p> OpenJDK</p> </li> <li> <p> Julia</p> <p>Required to run the CorrectMatch module with a thin Python wrapper.</p> <p> Julia</p> </li> </ul>"},{"location":"setup/correctmatch/","title":"CorrectMatch","text":"<p>Warning</p> <p>Requires Julia to run. This particular setup was tested with Julia version 1.8.5 on an M1 MBP.</p> <p>CorrectMatch is used via a thin Python wrapper for the Julia module CorrectMatch.jl to estimate uniqueness from small population samples.</p>"},{"location":"setup/correctmatch/#install-correctmatch-in-julia","title":"Install CorrectMatch in Julia","text":"Change working directory to privfp-experiments<pre><code>cd privfp-experiments\njulia\n</code></pre> Add CorrectMatch package<pre><code>using Pkg\nPkg.add(\"CorrectMatch\")\n</code></pre>"},{"location":"setup/correctmatch/#install-pycorrectmatch-dependencies","title":"Install PyCorrectMatch dependencies","text":"Create Python virtual environment<pre><code>python3.11 -m venv .venv\nsource .venv/bin/activate\n</code></pre> Install correctmatch package<pre><code>pip install correctmatch\npython\n</code></pre> Install Julia dependencies through virtual environment<pre><code>import julia\njulia.install()\n</code></pre>"},{"location":"setup/correctmatch/#verify-installlation","title":"Verify installlation","text":"Run correctmatch<pre><code>import numpy as np\nimport correctmatch\n\narr = np.random.randint(1, 5, size=(1000, 5))\ncorrectmatch.precompile()  # precompile the Julia module\ncorrectmatch.uniqueness(arr)  # true uniqueness for 1,000 records\n</code></pre>"},{"location":"setup/extraction/","title":"Entity extraction","text":"<p>Warning</p> <p>Requires the quantised <code>NER/UniNER-7B-type</code> model placed in the <code>model</code> folder.</p> <p>The quantised model was created by cloning the llama.cpp repository and quantising the <code>Universal-NER/UniNER-7B-type</code> locally to <code>q4_1.gguf</code> format.</p> <p>The llama.cpp repository has guidance on their repo in their Prepare and Quantize section. Alternatively there is a medium article that goes through all of this in a step-by-step process.</p> <p>For more information on quantising see here.</p>"},{"location":"setup/llm/","title":"Large Language Models","text":"<p>Warning</p> <p>Requires Ollama to run. This particular setup was tested with Ollama version 0.1.27 on an M1 MBP.</p> <p>Ollama is used for the unstructured generative component of Privacy Fingerprint. It provides a simple interface to download quantised models and run inference locally.</p>"},{"location":"setup/llm/#start-ollama","title":"Start Ollama","text":"<p>Either open up the desktop application or a terminal and enter <code>ollama serve</code>.</p>"},{"location":"setup/llm/#ollama-models","title":"Ollama models","text":"<p>To download a model open a terminal and enter <code>ollama pull &lt;model_name&gt;</code>. The example notebooks in this repository currently use <code>llama2:latest fe938a131f40</code>.</p> <p>See the Ollama model library for all available models.</p>"},{"location":"setup/llm/#other-models","title":"Other models","text":"<p>It is possible to use your own models not specified in the Ollama model library. Ollama supports the <code>.gguf</code> format and many quantised and non-quantised models can be found on the Hugging Face Hub.</p> <p>To quantise a model, check out the resources on setting up open source LLMs with llama.cpp and the introductory reading around quantisation.</p>"},{"location":"setup/synthea/","title":"Synthea International","text":"<p>Warning</p> <p>Requires OpenJDK to run. This particular setup was tested with Homebrew openjdk@17 on an M1 MBP.</p> <p>Synthea is used for the structured generative component of Privacy Fingerprint with UK adaptation from Synthea International.</p>"},{"location":"setup/synthea/#clone-repositories","title":"Clone repositories","text":"Open a terminal in a projects directory<pre><code>git clone https://github.com/nhsengland/privfp-experiments.git\ngit clone https://github.com/synthetichealth/synthea.git\ngit clone https://github.com/synthetichealth/synthea-international.git\n</code></pre>"},{"location":"setup/synthea/#modify-synthea","title":"Modify Synthea","text":"Checkout compatible versions<pre><code>cd synthea\ngit checkout fb43d3957762465211e70daeeb2bc00e0dbd1c1d\n\ncd ../synthea-international\ngit checkout b4cd9a30f106957ae8f2682090d45b04a49a7c4b\n</code></pre> Copy UK adaptation<pre><code>cp -R gb/* ../synthea\ncd ../synthea\n</code></pre>"},{"location":"setup/synthea/#verify-installation","title":"Verify installation","text":"Run Synthea<pre><code>./run_synthea \"West Yorkshire\"\n</code></pre>"}]}