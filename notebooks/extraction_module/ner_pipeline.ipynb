{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.extraction import extraction\n",
    "from src.standardisation import standardisation\n",
    "from src.data_imports.llm_ingestion import load_llm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_llm_data()[0:20]\n",
    "entities = [\"person\", \"NHS number\", \"date of birth\", \"diagnosis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./models/quantized_q4_1.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_1     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:              blk.0.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:              blk.0.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:         blk.0.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:            blk.0.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:              blk.0.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:              blk.1.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:              blk.1.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:              blk.1.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:         blk.1.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:            blk.1.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:              blk.1.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:              blk.2.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:              blk.2.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:              blk.2.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:         blk.2.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:            blk.2.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:              blk.2.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:              blk.3.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:            blk.3.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:              blk.3.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:              blk.4.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:              blk.4.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:              blk.4.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:         blk.4.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:            blk.4.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:              blk.4.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:              blk.5.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:              blk.5.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:              blk.5.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:         blk.5.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:            blk.5.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:              blk.5.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:              blk.6.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:              blk.6.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:              blk.6.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:         blk.6.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:            blk.6.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:              blk.6.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:              blk.7.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:              blk.7.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:              blk.7.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:         blk.7.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:            blk.7.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:              blk.7.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:              blk.8.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:         blk.8.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:            blk.8.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:              blk.8.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:              blk.9.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:              blk.9.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:              blk.9.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:         blk.9.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:            blk.9.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:              blk.9.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.10.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:             blk.10.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.10.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:        blk.10.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:           blk.10.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.10.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.11.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:             blk.11.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:             blk.11.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:        blk.11.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:           blk.11.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:             blk.11.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:             blk.12.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:             blk.12.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:             blk.12.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:        blk.12.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:           blk.12.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:             blk.12.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:             blk.13.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:             blk.13.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:             blk.13.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:        blk.13.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:           blk.13.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:             blk.13.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:             blk.14.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:             blk.14.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:             blk.14.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:        blk.14.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:           blk.14.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:             blk.14.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.15.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:             blk.15.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:             blk.15.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:        blk.15.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:           blk.15.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:             blk.15.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:             blk.16.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:             blk.16.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:             blk.16.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:        blk.16.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:           blk.16.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:             blk.16.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:             blk.17.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:        blk.17.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:             blk.17.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:             blk.18.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:             blk.18.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:             blk.18.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:        blk.18.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:           blk.18.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:             blk.18.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.19.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:             blk.19.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:             blk.19.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:        blk.19.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:           blk.19.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.19.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:             blk.20.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:             blk.20.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:             blk.20.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:        blk.20.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:           blk.20.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:             blk.20.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.21.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:           blk.21.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:             blk.21.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:             blk.22.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:             blk.22.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:             blk.22.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:        blk.22.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:           blk.22.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:             blk.22.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:             blk.23.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:             blk.23.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:             blk.23.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:        blk.23.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:           blk.23.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:             blk.23.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:             blk.24.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:             blk.24.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:             blk.24.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:        blk.24.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.25.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:             blk.25.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:             blk.25.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:        blk.25.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:             blk.26.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:        blk.26.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.27.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:             blk.27.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:             blk.27.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:        blk.27.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.28.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:             blk.28.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:             blk.28.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:        blk.28.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.29.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:             blk.29.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:             blk.29.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:        blk.29.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.30.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.31.attn_q.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:             blk.31.attn_k.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:             blk.31.attn_v.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:        blk.31.attn_output.weight q4_1     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_down.weight q4_1     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.ffn_up.weight q4_1     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 3\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_1:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_1\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.95 GiB (5.03 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
      "llm_load_tensors: mem required  = 4041.80 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1\n",
      "ggml_metal_init: picking default device: Apple M1\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/scarlettkynoch/Documents/Projects/privfp-experiments/.venv/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M1\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7 (1007)\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: maxTransferRate               = built-in GPU\n",
      "llama_new_context_with_model: compute buffer total size = 73.82 MiB\n",
      "llama_new_context_with_model: max tensor size =   102.54 MiB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  4042.39 MiB, ( 4044.02 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =   256.03 MiB, ( 4300.05 / 10922.67)\n",
      "ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =    70.52 MiB, ( 4370.56 / 10922.67)\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "/Users/scarlettkynoch/Documents/Projects/privfp-experiments/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Brandon McLaughlin\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.01 ms /     9 runs   (    0.11 ms per token,  8955.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8139.68 ms /   160 tokens (   50.87 ms per token,    19.66 tokens per second)\n",
      "llama_print_timings:        eval time =     796.92 ms /     8 runs   (   99.61 ms per token,    10.04 tokens per second)\n",
      "llama_print_timings:       total time =    8968.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"568 968 0803\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    15 runs   (    0.14 ms per token,  6902.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     269.03 ms /    25 tokens (   10.76 ms per token,    92.93 tokens per second)\n",
      "llama_print_timings:        eval time =    1269.98 ms /    14 runs   (   90.71 ms per token,    11.02 tokens per second)\n",
      "llama_print_timings:       total time =    1595.56 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"1963-12-07\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.40 ms /    13 runs   (    0.11 ms per token,  9319.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.22 ms /    25 tokens (   10.73 ms per token,    93.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1155.18 ms /    12 runs   (   96.26 ms per token,    10.39 tokens per second)\n",
      "llama_print_timings:       total time =    1453.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"acute bronchitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.02 ms /     8 runs   (    0.13 ms per token,  7843.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.93 ms /    24 tokens (   11.66 ms per token,    85.74 tokens per second)\n",
      "llama_print_timings:        eval time =     724.16 ms /     7 runs   (  103.45 ms per token,     9.67 tokens per second)\n",
      "llama_print_timings:       total time =    1022.84 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mr. Esteban Altenwerth\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.71 ms /    12 runs   (    0.14 ms per token,  7017.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1287.81 ms /   150 tokens (    8.59 ms per token,   116.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1252.98 ms /    11 runs   (  113.91 ms per token,     8.78 tokens per second)\n",
      "llama_print_timings:       total time =    2572.13 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"201 826 4805\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.79 ms /    15 runs   (    0.12 ms per token,  8393.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     278.20 ms /    25 tokens (   11.13 ms per token,    89.86 tokens per second)\n",
      "llama_print_timings:        eval time =    1331.11 ms /    14 runs   (   95.08 ms per token,    10.52 tokens per second)\n",
      "llama_print_timings:       total time =    1643.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"31st of May 1985\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.53 ms /    13 runs   (    0.12 ms per token,  8496.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     280.96 ms /    25 tokens (   11.24 ms per token,    88.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1272.84 ms /    12 runs   (  106.07 ms per token,     9.43 tokens per second)\n",
      "llama_print_timings:       total time =    1580.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     2 runs   (    0.15 ms per token,  6734.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.04 ms /    24 tokens (   11.42 ms per token,    87.58 tokens per second)\n",
      "llama_print_timings:        eval time =      96.84 ms /     1 runs   (   96.84 ms per token,    10.33 tokens per second)\n",
      "llama_print_timings:       total time =     375.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mr. Ervin O'Connell\", \"Dr. Randa Ritchie\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       3.50 ms /    20 runs   (    0.17 ms per token,  5715.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1293.30 ms /   155 tokens (    8.34 ms per token,   119.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1871.35 ms /    19 runs   (   98.49 ms per token,    10.15 tokens per second)\n",
      "llama_print_timings:       total time =    3237.95 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"862 334 6633\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.98 ms /    15 runs   (    0.47 ms per token,  2148.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     291.00 ms /    25 tokens (   11.64 ms per token,    85.91 tokens per second)\n",
      "llama_print_timings:        eval time =    1217.57 ms /    14 runs   (   86.97 ms per token,    11.50 tokens per second)\n",
      "llama_print_timings:       total time =    1603.68 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"1964-04-14\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.45 ms /    13 runs   (    0.34 ms per token,  2923.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.47 ms /    25 tokens (   10.54 ms per token,    94.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1230.92 ms /    12 runs   (  102.58 ms per token,     9.75 tokens per second)\n",
      "llama_print_timings:       total time =    1563.39 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"acute bronchitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.53 ms /     8 runs   (    0.19 ms per token,  5242.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.46 ms /    24 tokens (   11.64 ms per token,    85.88 tokens per second)\n",
      "llama_print_timings:        eval time =     807.26 ms /     7 runs   (  115.32 ms per token,     8.67 tokens per second)\n",
      "llama_print_timings:       total time =    1200.20 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Pam Boehm\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       3.08 ms /     8 runs   (    0.39 ms per token,  2594.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1246.20 ms /   135 tokens (    9.23 ms per token,   108.33 tokens per second)\n",
      "llama_print_timings:        eval time =     672.29 ms /     7 runs   (   96.04 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =    1968.86 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"639 189 7032\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.33 ms /    15 runs   (    0.36 ms per token,  2816.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.13 ms /    25 tokens (   10.69 ms per token,    93.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1350.05 ms /    14 runs   (   96.43 ms per token,    10.37 tokens per second)\n",
      "llama_print_timings:       total time =    1698.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.01 ms /     2 runs   (    0.50 ms per token,  1984.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.03 ms /    25 tokens (   10.72 ms per token,    93.27 tokens per second)\n",
      "llama_print_timings:        eval time =      97.03 ms /     1 runs   (   97.03 ms per token,    10.31 tokens per second)\n",
      "llama_print_timings:       total time =     377.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       0.69 ms /     2 runs   (    0.34 ms per token,  2906.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =     279.16 ms /    24 tokens (   11.63 ms per token,    85.97 tokens per second)\n",
      "llama_print_timings:        eval time =      94.21 ms /     1 runs   (   94.21 ms per token,    10.62 tokens per second)\n",
      "llama_print_timings:       total time =     382.18 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mrs. Mirta Mosciski\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.43 ms /    11 runs   (    0.40 ms per token,  2480.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1243.87 ms /   135 tokens (    9.21 ms per token,   108.53 tokens per second)\n",
      "llama_print_timings:        eval time =     832.32 ms /    10 runs   (   83.23 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =    2142.98 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"231 157 0705\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.17 ms /    15 runs   (    0.48 ms per token,  2093.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.69 ms /    25 tokens (   10.59 ms per token,    94.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1156.96 ms /    14 runs   (   82.64 ms per token,    12.10 tokens per second)\n",
      "llama_print_timings:       total time =    1516.85 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       0.93 ms /     2 runs   (    0.46 ms per token,  2152.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.53 ms /    25 tokens (   10.70 ms per token,    93.45 tokens per second)\n",
      "llama_print_timings:        eval time =      82.05 ms /     1 runs   (   82.05 ms per token,    12.19 tokens per second)\n",
      "llama_print_timings:       total time =     359.87 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"viral sinusitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       3.40 ms /     8 runs   (    0.43 ms per token,  2350.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.50 ms /    24 tokens (   11.02 ms per token,    90.74 tokens per second)\n",
      "llama_print_timings:        eval time =     680.92 ms /     7 runs   (   97.27 ms per token,    10.28 tokens per second)\n",
      "llama_print_timings:       total time =    1017.79 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mr.\", \"Pierre Hyatt\", \"Doctor Mercy Kuhlman\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.31 ms /    18 runs   (    0.41 ms per token,  2463.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1501.53 ms /   185 tokens (    8.12 ms per token,   123.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1486.16 ms /    17 runs   (   87.42 ms per token,    11.44 tokens per second)\n",
      "llama_print_timings:       total time =    3108.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"476 859 5816\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.26 ms /    15 runs   (    0.42 ms per token,  2395.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.06 ms /    25 tokens (   10.68 ms per token,    93.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1223.98 ms /    14 runs   (   87.43 ms per token,    11.44 tokens per second)\n",
      "llama_print_timings:       total time =    1581.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"1987-12-27\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.87 ms /    13 runs   (    0.37 ms per token,  2669.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.52 ms /    25 tokens (   10.62 ms per token,    94.16 tokens per second)\n",
      "llama_print_timings:        eval time =    1015.55 ms /    12 runs   (   84.63 ms per token,    11.82 tokens per second)\n",
      "llama_print_timings:       total time =    1349.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"viral sinusitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.23 ms /     8 runs   (    0.53 ms per token,  1892.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.36 ms /    24 tokens (   11.06 ms per token,    90.44 tokens per second)\n",
      "llama_print_timings:        eval time =     589.90 ms /     7 runs   (   84.27 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:       total time =     911.71 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Ms. Angela Collier\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.21 ms /    10 runs   (    0.42 ms per token,  2375.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1474.78 ms /   167 tokens (    8.83 ms per token,   113.24 tokens per second)\n",
      "llama_print_timings:        eval time =     779.75 ms /     9 runs   (   86.64 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =    2307.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"590 161 4272\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.62 ms /    15 runs   (    0.51 ms per token,  1969.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.97 ms /    25 tokens (   10.60 ms per token,    94.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1177.40 ms /    14 runs   (   84.10 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =    1542.44 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"1996-06-03\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.43 ms /    13 runs   (    0.57 ms per token,  1749.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.87 ms /    25 tokens (   10.59 ms per token,    94.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1009.53 ms /    12 runs   (   84.13 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =    1365.12 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Perennial allergic rhinitis with seasonal variation\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.58 ms /    16 runs   (    0.35 ms per token,  2865.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.65 ms /    24 tokens (   11.07 ms per token,    90.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1278.86 ms /    15 runs   (   85.26 ms per token,    11.73 tokens per second)\n",
      "llama_print_timings:       total time =    1632.58 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mr. Shannon Borer\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.28 ms /     9 runs   (    0.48 ms per token,  2104.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1223.52 ms /   148 tokens (    8.27 ms per token,   120.96 tokens per second)\n",
      "llama_print_timings:        eval time =     664.48 ms /     8 runs   (   83.06 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =    1947.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"122 273 9962\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.65 ms /    15 runs   (    0.44 ms per token,  2257.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.95 ms /    25 tokens (   10.56 ms per token,    94.71 tokens per second)\n",
      "llama_print_timings:        eval time =    1191.65 ms /    14 runs   (   85.12 ms per token,    11.75 tokens per second)\n",
      "llama_print_timings:       total time =    1541.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"1982-10-10\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.84 ms /    13 runs   (    0.53 ms per token,  1899.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.89 ms /    25 tokens (   10.56 ms per token,    94.74 tokens per second)\n",
      "llama_print_timings:        eval time =     999.08 ms /    12 runs   (   83.26 ms per token,    12.01 tokens per second)\n",
      "llama_print_timings:       total time =    1354.09 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Viral sinusitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.47 ms /     8 runs   (    0.56 ms per token,  1788.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.27 ms /    24 tokens (   11.05 ms per token,    90.47 tokens per second)\n",
      "llama_print_timings:        eval time =     590.58 ms /     7 runs   (   84.37 ms per token,    11.85 tokens per second)\n",
      "llama_print_timings:       total time =     909.15 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Delmy Halvorson\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.32 ms /     9 runs   (    0.48 ms per token,  2083.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1255.15 ms /   153 tokens (    8.20 ms per token,   121.90 tokens per second)\n",
      "llama_print_timings:        eval time =     672.45 ms /     8 runs   (   84.06 ms per token,    11.90 tokens per second)\n",
      "llama_print_timings:       total time =    1981.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"999 774 0304\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.60 ms /    15 runs   (    0.37 ms per token,  2679.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.39 ms /    25 tokens (   10.70 ms per token,    93.50 tokens per second)\n",
      "llama_print_timings:        eval time =    1287.84 ms /    14 runs   (   91.99 ms per token,    10.87 tokens per second)\n",
      "llama_print_timings:       total time =    1620.32 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"2005-12-12\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.48 ms /    13 runs   (    0.50 ms per token,  2004.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.77 ms /    25 tokens (   10.59 ms per token,    94.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1018.39 ms /    12 runs   (   84.87 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =    1370.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"acute viral pharyngitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.23 ms /    11 runs   (    0.57 ms per token,  1765.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.43 ms /    24 tokens (   11.02 ms per token,    90.76 tokens per second)\n",
      "llama_print_timings:        eval time =     839.90 ms /    10 runs   (   83.99 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:       total time =    1179.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mrs. Myra Schamberger\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.11 ms /    11 runs   (    0.37 ms per token,  2678.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1333.60 ms /   158 tokens (    8.44 ms per token,   118.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1038.72 ms /    10 runs   (  103.87 ms per token,     9.63 tokens per second)\n",
      "llama_print_timings:       total time =    2476.34 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"NHS number 855 311 5624\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =      10.23 ms /    19 runs   (    0.54 ms per token,  1857.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     331.81 ms /    25 tokens (   13.27 ms per token,    75.34 tokens per second)\n",
      "llama_print_timings:        eval time =    1653.12 ms /    18 runs   (   91.84 ms per token,    10.89 tokens per second)\n",
      "llama_print_timings:       total time =    2147.08 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"13 July 1979\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /    11 runs   (    0.40 ms per token,  2521.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     269.83 ms /    25 tokens (   10.79 ms per token,    92.65 tokens per second)\n",
      "llama_print_timings:        eval time =     947.00 ms /    10 runs   (   94.70 ms per token,    10.56 tokens per second)\n",
      "llama_print_timings:       total time =    1278.23 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       0.71 ms /     2 runs   (    0.35 ms per token,  2816.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.42 ms /    24 tokens (   12.93 ms per token,    77.31 tokens per second)\n",
      "llama_print_timings:        eval time =      90.54 ms /     1 runs   (   90.54 ms per token,    11.05 tokens per second)\n",
      "llama_print_timings:       total time =     409.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Cristy Robel\", \"Dr Emory Schuster\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.28 ms /    15 runs   (    0.35 ms per token,  2839.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1504.75 ms /   178 tokens (    8.45 ms per token,   118.29 tokens per second)\n",
      "llama_print_timings:        eval time =    1344.92 ms /    14 runs   (   96.07 ms per token,    10.41 tokens per second)\n",
      "llama_print_timings:       total time =    2940.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"999 251 4401\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       8.62 ms /    15 runs   (    0.57 ms per token,  1739.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =     284.17 ms /    25 tokens (   11.37 ms per token,    87.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1294.37 ms /    14 runs   (   92.46 ms per token,    10.82 tokens per second)\n",
      "llama_print_timings:       total time =    1675.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"2009-10-09\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.08 ms /    13 runs   (    0.47 ms per token,  2137.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.14 ms /    25 tokens (   10.65 ms per token,    93.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1153.75 ms /    12 runs   (   96.15 ms per token,    10.40 tokens per second)\n",
      "llama_print_timings:       total time =    1497.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Otitis media\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.03 ms /     7 runs   (    0.86 ms per token,  1160.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     282.09 ms /    24 tokens (   11.75 ms per token,    85.08 tokens per second)\n",
      "llama_print_timings:        eval time =     532.73 ms /     6 runs   (   88.79 ms per token,    11.26 tokens per second)\n",
      "llama_print_timings:       total time =     871.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Ms. Dorie Baumbach\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.71 ms /    11 runs   (    0.43 ms per token,  2334.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1531.92 ms /   167 tokens (    9.17 ms per token,   109.01 tokens per second)\n",
      "llama_print_timings:        eval time =     943.75 ms /    10 runs   (   94.38 ms per token,    10.60 tokens per second)\n",
      "llama_print_timings:       total time =    2533.52 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"447 249 2567\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       9.54 ms /    15 runs   (    0.64 ms per token,  1571.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     306.50 ms /    25 tokens (   12.26 ms per token,    81.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1488.84 ms /    14 runs   (  106.35 ms per token,     9.40 tokens per second)\n",
      "llama_print_timings:       total time =    1907.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"June 20, 1953\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.20 ms /    14 runs   (    0.51 ms per token,  1944.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.95 ms /    25 tokens (   10.68 ms per token,    93.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1190.00 ms /    13 runs   (   91.54 ms per token,    10.92 tokens per second)\n",
      "llama_print_timings:       total time =    1556.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"chronic sinusitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.81 ms /     8 runs   (    0.60 ms per token,  1663.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.16 ms /    24 tokens (   11.09 ms per token,    90.17 tokens per second)\n",
      "llama_print_timings:        eval time =     602.81 ms /     7 runs   (   86.12 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:       total time =     916.11 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Ms. Deidra Corkery\", \"Almeda Okuneva\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       9.70 ms /    20 runs   (    0.48 ms per token,  2062.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1768.44 ms /   202 tokens (    8.75 ms per token,   114.22 tokens per second)\n",
      "llama_print_timings:        eval time =    1792.84 ms /    19 runs   (   94.36 ms per token,    10.60 tokens per second)\n",
      "llama_print_timings:       total time =    3782.35 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"328 659 9956\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.92 ms /    15 runs   (    0.46 ms per token,  2168.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =     272.47 ms /    25 tokens (   10.90 ms per token,    91.75 tokens per second)\n",
      "llama_print_timings:        eval time =    1330.06 ms /    14 runs   (   95.00 ms per token,    10.53 tokens per second)\n",
      "llama_print_timings:       total time =    1707.25 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.09 ms /     2 runs   (    0.54 ms per token,  1841.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.34 ms /    25 tokens (   10.73 ms per token,    93.17 tokens per second)\n",
      "llama_print_timings:        eval time =      84.18 ms /     1 runs   (   84.18 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:       total time =     366.75 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.21 ms /     2 runs   (    0.60 ms per token,  1654.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.68 ms /    24 tokens (   11.07 ms per token,    90.33 tokens per second)\n",
      "llama_print_timings:        eval time =      82.08 ms /     1 runs   (   82.08 ms per token,    12.18 tokens per second)\n",
      "llama_print_timings:       total time =     361.97 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Wendell Oberbrunner\", \"Dr. Ivette Wunsch\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       8.76 ms /    20 runs   (    0.44 ms per token,  2282.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1476.25 ms /   178 tokens (    8.29 ms per token,   120.58 tokens per second)\n",
      "llama_print_timings:        eval time =    1656.98 ms /    19 runs   (   87.21 ms per token,    11.47 tokens per second)\n",
      "llama_print_timings:       total time =    3252.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"999 595 1633\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       8.59 ms /    15 runs   (    0.57 ms per token,  1746.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.23 ms /    25 tokens (   10.73 ms per token,    93.20 tokens per second)\n",
      "llama_print_timings:        eval time =    1195.64 ms /    14 runs   (   85.40 ms per token,    11.71 tokens per second)\n",
      "llama_print_timings:       total time =    1569.43 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"1958-11-22\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.51 ms /    13 runs   (    0.58 ms per token,  1731.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.42 ms /    25 tokens (   10.66 ms per token,    93.84 tokens per second)\n",
      "llama_print_timings:        eval time =    1032.61 ms /    12 runs   (   86.05 ms per token,    11.62 tokens per second)\n",
      "llama_print_timings:       total time =    1411.02 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       0.83 ms /     2 runs   (    0.42 ms per token,  2400.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.33 ms /    24 tokens (   11.47 ms per token,    87.17 tokens per second)\n",
      "llama_print_timings:        eval time =     109.37 ms /     1 runs   (  109.37 ms per token,     9.14 tokens per second)\n",
      "llama_print_timings:       total time =     396.19 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Ethan Kutch\", \"Dylan Robel\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.40 ms /    14 runs   (    0.39 ms per token,  2595.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1232.74 ms /   142 tokens (    8.68 ms per token,   115.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1190.72 ms /    13 runs   (   91.59 ms per token,    10.92 tokens per second)\n",
      "llama_print_timings:       total time =    2497.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"999 126 7519\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =      12.35 ms /    15 runs   (    0.82 ms per token,  1214.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.96 ms /    25 tokens (   10.68 ms per token,    93.65 tokens per second)\n",
      "llama_print_timings:        eval time =    1214.61 ms /    14 runs   (   86.76 ms per token,    11.53 tokens per second)\n",
      "llama_print_timings:       total time =    1582.16 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"2015-08-06\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.49 ms /    13 runs   (    0.50 ms per token,  2003.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.25 ms /    25 tokens (   10.73 ms per token,    93.20 tokens per second)\n",
      "llama_print_timings:        eval time =    1039.99 ms /    12 runs   (   86.67 ms per token,    11.54 tokens per second)\n",
      "llama_print_timings:       total time =    1387.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.52 ms /     2 runs   (    0.76 ms per token,  1318.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.51 ms /    24 tokens (   11.02 ms per token,    90.73 tokens per second)\n",
      "llama_print_timings:        eval time =      82.86 ms /     1 runs   (   82.86 ms per token,    12.07 tokens per second)\n",
      "llama_print_timings:       total time =     362.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mr. Lupe Cassin\", \"Dr. Michel Zulauf\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       8.21 ms /    17 runs   (    0.48 ms per token,  2070.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1244.13 ms /   157 tokens (    7.92 ms per token,   126.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1398.25 ms /    16 runs   (   87.39 ms per token,    11.44 tokens per second)\n",
      "llama_print_timings:       total time =    2787.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"120 506 4336\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.81 ms /    15 runs   (    0.45 ms per token,  2202.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.18 ms /    25 tokens (   10.57 ms per token,    94.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1238.94 ms /    14 runs   (   88.50 ms per token,    11.30 tokens per second)\n",
      "llama_print_timings:       total time =    1593.69 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       1.16 ms /     2 runs   (    0.58 ms per token,  1727.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.99 ms /    25 tokens (   11.00 ms per token,    90.91 tokens per second)\n",
      "llama_print_timings:        eval time =      90.22 ms /     1 runs   (   90.22 ms per token,    11.08 tokens per second)\n",
      "llama_print_timings:       total time =     378.51 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"bacterial sinusitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       3.61 ms /     9 runs   (    0.40 ms per token,  2495.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.71 ms /    24 tokens (   11.15 ms per token,    89.65 tokens per second)\n",
      "llama_print_timings:        eval time =     670.19 ms /     8 runs   (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =     991.55 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mrs. Logan Schaden\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       6.12 ms /    10 runs   (    0.61 ms per token,  1633.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1222.93 ms /   143 tokens (    8.55 ms per token,   116.93 tokens per second)\n",
      "llama_print_timings:        eval time =     757.34 ms /     9 runs   (   84.15 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:       total time =    2047.14 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"169 421 2926\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.26 ms /    15 runs   (    0.48 ms per token,  2065.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     267.64 ms /    25 tokens (   10.71 ms per token,    93.41 tokens per second)\n",
      "llama_print_timings:        eval time =    1187.72 ms /    14 runs   (   84.84 ms per token,    11.79 tokens per second)\n",
      "llama_print_timings:       total time =    1543.74 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"1958-11-05\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       9.16 ms /    13 runs   (    0.70 ms per token,  1419.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.05 ms /    25 tokens (   10.64 ms per token,    93.97 tokens per second)\n",
      "llama_print_timings:        eval time =    1005.23 ms /    12 runs   (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =    1366.26 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       0.86 ms /     2 runs   (    0.43 ms per token,  2314.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.81 ms /    24 tokens (   11.03 ms per token,    90.63 tokens per second)\n",
      "llama_print_timings:        eval time =      83.77 ms /     1 runs   (   83.77 ms per token,    11.94 tokens per second)\n",
      "llama_print_timings:       total time =     359.36 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Mr. Jaime Sauer\", \"Seth Dare\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.62 ms /    15 runs   (    0.37 ms per token,  2669.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1221.69 ms /   145 tokens (    8.43 ms per token,   118.69 tokens per second)\n",
      "llama_print_timings:        eval time =    1218.18 ms /    14 runs   (   87.01 ms per token,    11.49 tokens per second)\n",
      "llama_print_timings:       total time =    2519.77 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"355 112 3923\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.91 ms /    15 runs   (    0.53 ms per token,  1897.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.11 ms /    25 tokens (   11.00 ms per token,    90.87 tokens per second)\n",
      "llama_print_timings:        eval time =    1199.93 ms /    14 runs   (   85.71 ms per token,    11.67 tokens per second)\n",
      "llama_print_timings:       total time =    1580.57 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"26th of January 1959\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.76 ms /    13 runs   (    0.37 ms per token,  2731.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.10 ms /    25 tokens (   10.72 ms per token,    93.25 tokens per second)\n",
      "llama_print_timings:        eval time =    1033.11 ms /    12 runs   (   86.09 ms per token,    11.62 tokens per second)\n",
      "llama_print_timings:       total time =    1373.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"viral sinusitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.48 ms /     8 runs   (    0.56 ms per token,  1785.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     262.58 ms /    24 tokens (   10.94 ms per token,    91.40 tokens per second)\n",
      "llama_print_timings:        eval time =     591.14 ms /     7 runs   (   84.45 ms per token,    11.84 tokens per second)\n",
      "llama_print_timings:       total time =     911.29 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Reggie Langosh\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.36 ms /     7 runs   (    0.62 ms per token,  1606.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1216.52 ms /   134 tokens (    9.08 ms per token,   110.15 tokens per second)\n",
      "llama_print_timings:        eval time =     498.29 ms /     6 runs   (   83.05 ms per token,    12.04 tokens per second)\n",
      "llama_print_timings:       total time =    1766.54 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"999 367 0222\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       7.68 ms /    15 runs   (    0.51 ms per token,  1953.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.27 ms /    25 tokens (   10.65 ms per token,    93.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1173.52 ms /    14 runs   (   83.82 ms per token,    11.93 tokens per second)\n",
      "llama_print_timings:       total time =    1544.65 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"17/02/1990\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.31 ms /    13 runs   (    0.41 ms per token,  2447.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     262.93 ms /    25 tokens (   10.52 ms per token,    95.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1031.64 ms /    12 runs   (   85.97 ms per token,    11.63 tokens per second)\n",
      "llama_print_timings:       total time =    1366.47 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"viral sinusitis\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       4.82 ms /     8 runs   (    0.60 ms per token,  1659.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.52 ms /    24 tokens (   11.02 ms per token,    90.73 tokens per second)\n",
      "llama_print_timings:        eval time =     612.50 ms /     7 runs   (   87.50 ms per token,    11.43 tokens per second)\n",
      "llama_print_timings:       total time =     935.40 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"Ms. Luann Hills\", \"Florine Windler\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       9.39 ms /    16 runs   (    0.59 ms per token,  1703.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1724.88 ms /   194 tokens (    8.89 ms per token,   112.47 tokens per second)\n",
      "llama_print_timings:        eval time =    1262.03 ms /    15 runs   (   84.14 ms per token,    11.89 tokens per second)\n",
      "llama_print_timings:       total time =    3101.89 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"999 395 1923\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =      10.48 ms /    15 runs   (    0.70 ms per token,  1430.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =     268.06 ms /    25 tokens (   10.72 ms per token,    93.26 tokens per second)\n",
      "llama_print_timings:        eval time =    1312.43 ms /    14 runs   (   93.75 ms per token,    10.67 tokens per second)\n",
      "llama_print_timings:       total time =    1731.41 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"19-year-old\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       5.46 ms /     9 runs   (    0.61 ms per token,  1646.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     266.27 ms /    25 tokens (   10.65 ms per token,    93.89 tokens per second)\n",
      "llama_print_timings:        eval time =     676.40 ms /     8 runs   (   84.55 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:       total time =    1020.72 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [\"chief complaint\"]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    8139.79 ms\n",
      "llama_print_timings:      sample time =       3.75 ms /     7 runs   (    0.54 ms per token,  1867.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =     265.58 ms /    24 tokens (   11.07 ms per token,    90.37 tokens per second)\n",
      "llama_print_timings:        eval time =     509.21 ms /     6 runs   (   84.87 ms per token,    11.78 tokens per second)\n",
      "llama_print_timings:       total time =     821.74 ms\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "patient_entities = extraction.run(data, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person</th>\n",
       "      <th>NHS number</th>\n",
       "      <th>date of birth</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person  NHS number  date of birth  diagnosis\n",
       "0        0           9              6          4\n",
       "1        6           3             15          0\n",
       "2        5          12              7          4\n",
       "3       17          11              0          0\n",
       "4       11           4              0          9\n",
       "5        4           8              9          9\n",
       "6       13          10             10          2\n",
       "7        9           1              8          3\n",
       "8        2          18             11          5\n",
       "9       12          19              1          0\n",
       "10       1          14             12          1\n",
       "11      15           7             16          8\n",
       "12      14           5              0          0\n",
       "13      19          17              5          0\n",
       "14       3          13             13          0\n",
       "15       8           0              0          6\n",
       "16      10           2              4          0\n",
       "17       7           6             14          9\n",
       "18      18          15              2          9\n",
       "19      16          16              3          7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardisation.run(patient_entities, entities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
