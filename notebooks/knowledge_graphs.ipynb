{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "if path_root not in sys.path:\n",
    "    sys.path.append(path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import (\n",
    "    PyPDFLoader,\n",
    "    UnstructuredPDFLoader,\n",
    "    PyPDFium2Loader,\n",
    ")\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "llm_path = \"../example_output/example_pipeline_14_05_24/llm.json\"\n",
    "llm_output = load_json(path=llm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Clinical Note for Jeremiah Bednar:\\n\\nPatient presents with symptoms of acute viral pharyngitis, including sore throat, fever, and difficulty swallowing. Patient's NHS number is 4798819344 and date of birth is January 1, 2009. Diagnosis is acute viral pharyngitis (disorder).\"),\n",
       " Document(page_content='Here is an example clinical note for Temeka McCullough:\\n\\nPatient: Temeka McCullough\\nNHS Number: 1766306217\\nDate of Birth: September 14, 2000\\n\\nPresentation: Acute viral pharyngitis (disorder)\\n\\nNotes:\\n\\n* History of sore throat and fever for the past 2 days\\n*Difficulty swallowing and speaking\\n*Slight difficulty breathing\\n*No recent travel or exposure to sick individuals\\n*No known allergies\\n\\nPlan:\\n\\n*Prescribe antiviral medication as per protocol\\n*Advise patient on proper fluid intake and rest\\n*Monitor temperature and symptoms closely\\n\\nReason for visit: Acute viral pharyngitis (disorder)'),\n",
       " Document(page_content='Clinical Note for Patient Huey Orn:\\n\\nPatient presents with viral sinusitis, manifested by persistent nasal congestion, facial pain, and yellow discharge from the nose. History of fever and cough in the past 48 hours. No significant medical history or allergies. Current medications include acetaminophen for fever and ibuprofen for pain.'),\n",
       " Document(page_content='Certainly! Here is an example clinical note for Mr. Bennett Muller based on the provided information:\\n\\n\"Patient presents with chronic intractable migraine without aura, NHS number 0030587050, date of birth November 14, 1991. Headaches are severe and persistent, occurring daily for the past 6 months. No known triggers or previous treatments have been effective in managing symptoms.\"'),\n",
       " Document(page_content=\"Clinical Note:\\nPatient Eun Wolf, age 64, presents with acute viral pharyngitis. Symptoms include sore throat, fever, and difficulty swallowing. Patient's medical history includes hypertension and recent upper respiratory tract infection.\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_chunk_content_documents(\n",
    "    content: List[str], token_limit: int = 8000\n",
    ") -> List[Document]:\n",
    "    \"\"\"Chunks the documents up with an index for a given max token limit.\n",
    "\n",
    "    Args:\n",
    "        content (Dict): This is a dictionary of the content given with their respective indexes.\n",
    "        token_limit (int, optional): This is the max tokens documents are split up into. Defaults to 8000.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of documents with the required token amount.\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\"], chunk_size=token_limit, chunk_overlap=0\n",
    "    )\n",
    "    docs = text_splitter.create_documents(content)\n",
    "    return docs\n",
    "\n",
    "\n",
    "docs = get_chunk_content_documents(content=llm_output, token_limit=600)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from yachalk import chalk\n",
    "import json\n",
    "from src.ollama import client\n",
    "\n",
    "\n",
    "def documents2Dataframe(documents) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for chunk in documents:\n",
    "        row = {\n",
    "            \"text\": chunk.page_content,\n",
    "            **chunk.metadata,\n",
    "            \"chunk_id\": uuid.uuid4().hex,\n",
    "        }\n",
    "        rows = rows + [row]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "def df2ConceptsList(dataframe: pd.DataFrame) -> list:\n",
    "    # dataframe.reset_index(inplace=True)\n",
    "    results = dataframe.apply(\n",
    "        lambda row: extractConcepts(\n",
    "            row.text, {\"chunk_id\": row.chunk_id, \"type\": \"concept\"}\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    # invalid json results in NaN\n",
    "    results = results.dropna()\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities.\n",
    "    concept_list = np.concatenate(results).ravel().tolist()\n",
    "    return concept_list\n",
    "\n",
    "\n",
    "def concepts2Df(concepts_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    concepts_dataframe = pd.DataFrame(concepts_list).replace(\" \", np.nan)\n",
    "    concepts_dataframe = concepts_dataframe.dropna(subset=[\"entity\"])\n",
    "    concepts_dataframe[\"entity\"] = concepts_dataframe[\"entity\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    return concepts_dataframe\n",
    "\n",
    "\n",
    "def df2Graph(dataframe: pd.DataFrame, model=None) -> list:\n",
    "    # dataframe.reset_index(inplace=True)\n",
    "    results = dataframe.apply(\n",
    "        lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model),\n",
    "        axis=1,\n",
    "    )\n",
    "    # invalid json results in NaN\n",
    "    results = results.dropna()\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities.\n",
    "    concept_list = np.concatenate(results).ravel().tolist()\n",
    "    return concept_list\n",
    "\n",
    "\n",
    "def graph2Df(nodes_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    graph_dataframe = pd.DataFrame(nodes_list).replace(\" \", np.nan)\n",
    "    graph_dataframe = graph_dataframe.dropna(subset=[\"node_1\", \"node_2\"])\n",
    "    graph_dataframe[\"node_1\"] = graph_dataframe[\"node_1\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "    graph_dataframe[\"node_2\"] = graph_dataframe[\"node_2\"].apply(\n",
    "        lambda x: x.lower()\n",
    "    )\n",
    "\n",
    "    return graph_dataframe\n",
    "\n",
    "\n",
    "def extractConcepts(prompt: str, metadata={}, model=\"mistral-openorca:latest\"):\n",
    "    SYS_PROMPT = (\n",
    "        \"Your task is extract the key concepts (and non personal entities) mentioned in the given context. \"\n",
    "        \"Extract only the most important and atomistic concepts, if  needed break the concepts down to the simpler concepts.\"\n",
    "        \"Categorize the concepts in one of the following categories: \"\n",
    "        \"[event, concept, place, object, document, organisation, condition, misc]\\n\"\n",
    "        \"Format your output as a list of json with the following format:\\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"entity\": The Concept,\\n'\n",
    "        '       \"importance\": The concontextual importance of the concept on a scale of 1 to 5 (5 being the highest),\\n'\n",
    "        '       \"category\": The Type of Concept,\\n'\n",
    "        \"   }, \\n\"\n",
    "        \"{ }, \\n\"\n",
    "        \"]\\n\"\n",
    "    )\n",
    "    response, _ = client.generate(\n",
    "        model_name=model, system=SYS_PROMPT, prompt=prompt\n",
    "    )\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "    except:\n",
    "        print(\"\\n\\nERROR ### Here is the buggy response: \", response, \"\\n\\n\")\n",
    "        result = None\n",
    "    return result\n",
    "\n",
    "\n",
    "def graphPrompt(input: str, metadata={}, model=\"mistral-openorca:latest\"):\n",
    "    if model == None:\n",
    "        model = \"mistral-openorca:latest\"\n",
    "\n",
    "    # model_info = client.show(model_name=model)\n",
    "    # print( chalk.blue(model_info))\n",
    "\n",
    "    SYS_PROMPT = (\n",
    "        \"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a context chunk (delimited by ```) Your task is to extract the ontology \"\n",
    "        \"of terms mentioned in the given context. These terms should represent the key concepts as per the context. \\n\"\n",
    "        \"Thought 1: While traversing through each sentence, Think about the key terms mentioned in it.\\n\"\n",
    "        \"\\tTerms may include object, entity, location, organization, person, \\n\"\n",
    "        \"\\tcondition, acronym, documents, service, concept, etc.\\n\"\n",
    "        \"\\tTerms should be as atomistic as possible\\n\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one on one relation with other terms.\\n\"\n",
    "        \"\\tTerms that are mentioned in the same sentence or the same paragraph are typically related to each other.\\n\"\n",
    "        \"\\tTerms can be related to many other terms\\n\\n\"\n",
    "        \"Thought 3: Find out the relation between each such related pair of terms. \\n\\n\"\n",
    "        \"Format your output as a list of json. Each element of the list contains a pair of terms\"\n",
    "        \"and the relation between them, like the follwing: \\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"node_1\": \"A concept from extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from extracted ontology\",\\n'\n",
    "        '       \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\\n'\n",
    "        \"   }, {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "    response, _ = client.generate(\n",
    "        model_name=model, system=SYS_PROMPT, prompt=USER_PROMPT\n",
    "    )\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "    except:\n",
    "        print(\"\\n\\nERROR ### Here is the buggy response: \", response, \"\\n\\n\")\n",
    "        result = None\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
