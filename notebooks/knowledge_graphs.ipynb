{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "if path_root not in sys.path:\n",
    "    sys.path.append(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of going from Document to Graph\n",
    "\n",
    "### Text2Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import os\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
    "kg_tokens_dict = [\"<H>\", \"<R>\", \"<T>\"]\n",
    "num_added_toks = tokenizer.add_tokens(kg_tokens_dict)\n",
    "text_prefix = \"TEXT: \"\n",
    "graph_prefix = \"GRAPH: \"\n",
    "\n",
    "model_path = \"../models/webNLG_model.pkl\"\n",
    "device = torch.device(\"mps\")\n",
    "model = torch.load(model_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_triple(content):\n",
    "    entity_ls = set(\n",
    "        [\n",
    "            _e.strip()\n",
    "            for _e in list(\n",
    "                set(\n",
    "                    re.findall(r\"\\s*<H>([\\s\\w\\.\\/\\-]+)[<$]*\", content)\n",
    "                    + re.findall(r\"\\s*<R>([\\s\\w\\.\\/\\-]+)[<$]*\", content)\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hrt_ls = set(\n",
    "        [\n",
    "            (_r[0].strip(), _r[1].strip(), _r[2].strip())\n",
    "            for _r in re.findall(r\"<H>([^<]+)<R>([^<]+)<T>([^<]+)\", content)\n",
    "        ]\n",
    "    )\n",
    "    return entity_ls, hrt_ls\n",
    "\n",
    "\n",
    "def gen_json_response(hrt_ls):\n",
    "    \"\"\"\n",
    "    {\"graph\": { \"nodes\": [ { \"id\": 1, \"label\": \"Bob\", \"color\": \"#ffffff\" }, { \"id\": 2, \"label\": \"Alice\", \"color\": \"#ff7675\" } ],\n",
    "        \"edges\": [ { \"from\": 1, \"to\": 2, \"label\": \"roommate\" }, ] } }\n",
    "    \"\"\"\n",
    "    graph = {\"nodes\": [], \"edges\": []}\n",
    "    node_id = 0\n",
    "    node_dict = {}\n",
    "    for _h, _r, _t in hrt_ls:\n",
    "        if _h not in node_dict:\n",
    "            node_dict[_h] = node_id\n",
    "            graph[\"nodes\"].append(\n",
    "                {\"id\": node_id, \"label\": _h, \"color\": \"#ffffff\"}\n",
    "            )\n",
    "            node_id += 1\n",
    "        if _t not in node_dict:\n",
    "            node_dict[_t] = node_id\n",
    "            graph[\"nodes\"].append(\n",
    "                {\"id\": node_id, \"label\": _t, \"color\": \"#ffffff\"}\n",
    "            )\n",
    "            node_id += 1\n",
    "        graph[\"edges\"].append(\n",
    "            {\"from\": node_dict[_h], \"to\": node_dict[_t], \"label\": _r}\n",
    "        )\n",
    "    return {\"graph\": graph}\n",
    "\n",
    "\n",
    "def get_graph(text: str):\n",
    "    start = time.time()\n",
    "    input_content = text\n",
    "    prefix = text_prefix\n",
    "\n",
    "    input_content_tmp = tokenizer(\n",
    "        prefix + input_content,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=500,\n",
    "    )\n",
    "    input_ids = input_content_tmp.input_ids.to(\"mps\")\n",
    "    am = input_content_tmp.attention_mask.to(\"mps\")\n",
    "\n",
    "    model_outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=am,\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        max_length=500,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    out_content = tokenizer.decode(model_outputs[0], skip_special_tokens=True)\n",
    "    if \"<H>\" in out_content:\n",
    "        entity_pool, hrt_pool = parse_triple(out_content)\n",
    "        print(\"-----Graph-----\")\n",
    "        data = gen_json_response(hrt_pool)\n",
    "    else:\n",
    "        print(out_content)\n",
    "        data = {\"graph\": {\"nodes\": [], \"edges\": []}}\n",
    "\n",
    "    return {\"time\": time.time() - start, \"data\": data}\n",
    "\n",
    "\n",
    "def get_text_size(text, fontsize=10):\n",
    "    \"\"\"Estimate the size of the text in display coordinates.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    text_artist = plt.text(0, 0, text, fontsize=fontsize)\n",
    "    renderer = fig.canvas.get_renderer()\n",
    "    bbox = text_artist.get_window_extent(renderer=renderer)\n",
    "    plt.close(fig)\n",
    "    return bbox.width, bbox.height\n",
    "\n",
    "\n",
    "def draw_graph(data):\n",
    "    # Extract nodes and edges from the data\n",
    "    nodes = data[\"data\"][\"graph\"][\"nodes\"]\n",
    "    edges = data[\"data\"][\"graph\"][\"edges\"]\n",
    "\n",
    "    # Create a NetworkX graph\n",
    "    G = (\n",
    "        nx.DiGraph()\n",
    "    )  # DiGraph for a directed graph, use Graph() for undirected\n",
    "\n",
    "    # Add nodes\n",
    "    for node in nodes:\n",
    "        G.add_node(\n",
    "            node[\"id\"], label=node[\"label\"], color=node.get(\"color\", \"#0000FF\")\n",
    "        )\n",
    "\n",
    "    # Add edges\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[\"from\"], edge[\"to\"], label=edge[\"label\"])\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(G)  # Positions for all nodes\n",
    "\n",
    "    # Define a transparent blue color\n",
    "    transparent_blue = (0, 0, 1, 0.5)  # RGB (0, 0, 1) with 0.5 transparency\n",
    "\n",
    "    # Extract node colors, default to transparent blue\n",
    "    # node_colors = [node[1].get('color', transparent_blue) for node in G.nodes(data=True)]\n",
    "    node_colors = [transparent_blue for node in G.nodes(data=True)]\n",
    "\n",
    "    # Create a figure and set its size\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Draw nodes as ellipses\n",
    "    for node, (x, y) in pos.items():\n",
    "        label = G.nodes[node][\"label\"]\n",
    "        width, height = get_text_size(label, fontsize=10)\n",
    "        width /= 100  # Scale width to fit the graph\n",
    "        ellipse = patches.Ellipse(\n",
    "            (x, y),\n",
    "            width=width + 0.2,\n",
    "            height=0.1,\n",
    "            color=node_colors[node],\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax.add_patch(ellipse)\n",
    "        ax.text(\n",
    "            x,\n",
    "            y,\n",
    "            label,\n",
    "            horizontalalignment=\"center\",\n",
    "            verticalalignment=\"center\",\n",
    "        )\n",
    "\n",
    "    # Draw edges with increased width\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, arrowstyle=\"->\", arrowsize=10, ax=ax, width=2\n",
    "    )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = nx.get_edge_attributes(G, \"label\")\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "\n",
    "    # Remove axes\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # Display the graph\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_json\n",
    "\n",
    "llm_path = \"../example_output/example_pipeline_14_05_24/llm.json\"\n",
    "llm_output = load_json(llm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph(text=llm_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_graph(text=llm_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_graph(text=llm_output[2])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_graph(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REBEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "triplet_extractor = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"Babelscape/rebel-large\",\n",
    "    tokenizer=\"Babelscape/rebel-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text  Clinical Note for Patient Huey Orn:\n",
      "\n",
      "Patient presents with viral sinusitis, manifested by persistent nasal congestion, facial pain, and yellow discharge from the nose. History of fever and cough in the past 48 hours. No significant medical history or allergies. Current medications include acetaminophen for fever and ibuprofen for pain.\n",
      "Triplets [{'head': 'acetaminophen', 'type': 'medical condition treated', 'tail': 'fever'}, {'head': 'fever', 'type': 'drug used for treatment', 'tail': 'acetaminophen'}, {'head': 'ibuprofen', 'type': 'medical condition treated', 'tail': 'pain'}, {'head': 'pain', 'type': 'drug used for treatment', 'tail': 'ibuprofen'}]\n"
     ]
    }
   ],
   "source": [
    "clinical_text = llm_output[2]\n",
    "\n",
    "# We need to use the tokenizer manually since we need special tokens.\n",
    "extracted_text = triplet_extractor.tokenizer.batch_decode(\n",
    "    [\n",
    "        triplet_extractor(\n",
    "            clinical_text, return_tensors=True, return_text=False\n",
    "        )[0][\"generated_token_ids\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Function to parse the generated text and extract the triplets\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = \"\", \"\", \"\", \"\"\n",
    "    text = text.strip()\n",
    "    current = \"x\"\n",
    "    for token in (\n",
    "        text.replace(\"<s>\", \"\")\n",
    "        .replace(\"<pad>\", \"\")\n",
    "        .replace(\"</s>\", \"\")\n",
    "        .split()\n",
    "    ):\n",
    "        if token == \"<triplet>\":\n",
    "            current = \"t\"\n",
    "            if relation != \"\":\n",
    "                triplets.append(\n",
    "                    {\n",
    "                        \"head\": subject.strip(),\n",
    "                        \"type\": relation.strip(),\n",
    "                        \"tail\": object_.strip(),\n",
    "                    }\n",
    "                )\n",
    "                relation = \"\"\n",
    "            subject = \"\"\n",
    "        elif token == \"<subj>\":\n",
    "            current = \"s\"\n",
    "            if relation != \"\":\n",
    "                triplets.append(\n",
    "                    {\n",
    "                        \"head\": subject.strip(),\n",
    "                        \"type\": relation.strip(),\n",
    "                        \"tail\": object_.strip(),\n",
    "                    }\n",
    "                )\n",
    "            object_ = \"\"\n",
    "        elif token == \"<obj>\":\n",
    "            current = \"o\"\n",
    "            relation = \"\"\n",
    "        else:\n",
    "            if current == \"t\":\n",
    "                subject += \" \" + token\n",
    "            elif current == \"s\":\n",
    "                object_ += \" \" + token\n",
    "            elif current == \"o\":\n",
    "                relation += \" \" + token\n",
    "    if subject != \"\" and relation != \"\" and object_ != \"\":\n",
    "        triplets.append(\n",
    "            {\n",
    "                \"head\": subject.strip(),\n",
    "                \"type\": relation.strip(),\n",
    "                \"tail\": object_.strip(),\n",
    "            }\n",
    "        )\n",
    "    return triplets\n",
    "\n",
    "\n",
    "extracted_triplets = extract_triplets(extracted_text[0])\n",
    "\n",
    "print(\"Text\", clinical_text)\n",
    "print(\"Triplets\", extracted_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of going from Entities Extracted to Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_json\n",
    "\n",
    "extraction_path = \"../example_output/example_pipeline_14_05_24/extraction.json\"\n",
    "extraction_output = load_json(extraction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_list = [\"person\", \"diagnosis\", \"nhs number\", \"date of birth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = extraction_output[0][\"Entities\"]\n",
    "# Initialize a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add a starter node (document ID)\n",
    "doc_id = \"doc_1\"\n",
    "G.add_node(doc_id, label=\"document\")\n",
    "\n",
    "# Add nodes and edges based on the data\n",
    "for entry in data:\n",
    "    node_label = entry[\"label\"]\n",
    "    node_text = entry[\"text\"]\n",
    "    node_score = entry[\"score\"]\n",
    "\n",
    "    G.add_node(node_text, label=node_label, score=node_score)\n",
    "    G.add_edge(doc_id, node_text, label=node_label)\n",
    "\n",
    "nx.draw(G, with_labels=True)\n",
    "nt = Network(\"500px\", \"500px\")\n",
    "nt.from_nx(G)\n",
    "nt.show(\"nx.html\", notebook=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
