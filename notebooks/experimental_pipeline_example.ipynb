{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example PrivacyFingerprint Experiment Pipeline\n",
    "\n",
    "This notebook walks you through a potential experimental workflow, to introduce a user to how they can use the ExperimentalConfigHandler to test out various experimental workflows.\n",
    "\n",
    "This pipeline will look at the difference in extraction outputs when using:\n",
    "- GliNER\n",
    "- UniversalNER hosted locally\n",
    "- UniversalNER hosted via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "path_root = os.path.dirname(os.getcwd())\n",
    "\n",
    "if path_root not in sys.path:\n",
    "    sys.path.append(path_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary functions from ./src folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed for the experimental config handler.\n",
    "from src.config.experimental_config_handler import ExperimentalConfigHandler\n",
    "from src.config.global_config import load_global_config\n",
    "\n",
    "# Functions needed to create prompt templates and save them for the experiments.\n",
    "from src.config.prompt_template_handler import (\n",
    "    save_extraction_template_to_json,\n",
    "    save_generate_template_to_json,\n",
    "    load_and_validate_extraction_prompt_template,\n",
    "    load_and_validate_generate_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Loading Global and Experimental Config\n",
    "\n",
    "**global_config_path**  this is the location of the global config path and then the output folder name is redefined to ensure the example experiments are out in the open. (Normally the default output folder should be used for your own experiments.)\n",
    "\n",
    "**default_config_path** is given so the user can point to the default experimental config values. Currently the pipeline copies the original experimental config down into the folder, and if this exists, only uses the experimental config defined in that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the location of the experimental config file you want to copy.\n",
    "global_config_path = \"../config/global_config.yaml\"\n",
    "global_config = load_global_config(global_config_path)\n",
    "global_config.output_paths.output_folder = \"../example_output\"\n",
    "\n",
    "# Defines the location of the default experimental config file you want the experimental config to have defaults using.\n",
    "default_config_path = \"../config/experimental_config.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to define you overrides: These are both optional parameters if you do not define any, the pipeline will run with the defaults provided in the experimental pipeline.\n",
    "\n",
    "\n",
    "* **iter_overrides**: These are parameters where you want all unique parameters in each list defined to be combined together. \n",
    "\n",
    "    * For example: if \"var1\" = [\"a\", \"b\"] and var2 = [\"c\", \"d\"]. It would produces combinations = [[\"a\",\"c\"], [\"a\", \"d\"], [\"b\", \"c\"], [\"b\",\"d\"]]\n",
    "\n",
    "\n",
    "\n",
    "* **combine_overrides**: These are parameters where you want to combine two various parameters and only affect extraction.gliner_features, extraction.ollama_features, and extraction.local_features. \n",
    "\n",
    "    * For example: if \"model_name\" = [\"model1\", \"model2\"] and \"prompt_template\" = [\"promp_template1\",\"prompt_template2\"]. It would produce combinations [[\"model1\", \"prompt_template1\"][\"model2\", \"prompt_template2\"]]\n",
    "    * The values defined in each list need to be the same length otherwise you will get an error. \n",
    "\n",
    "\n",
    "\n",
    "You will **NEED** to define your experiment name as this defines where your experiment folder should sit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your iter overrides.\n",
    "iter_overrides = {\n",
    "    \"outputs.experiment_name\": \"example_experiment_17_06_24\",\n",
    "    \"extraction.server_model_type\": [\"gliner\", \"ollama\", \"local\"],\n",
    "}\n",
    "\n",
    "# Define your combine overrides.\n",
    "combine_overrides = {\n",
    "    \"generate.llm_model_features.llm_model_name\": [\"llama2\", \"llama3\"],\n",
    "    \"generate.llm_model_features.prompt_template_path\": [\n",
    "        \"llama2_template.json\",\n",
    "        \"llama3_template.json\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# This initialises the experimental config handler.\n",
    "config_handler = ExperimentalConfigHandler(\n",
    "    default_config_path=default_config_path,\n",
    "    iter_overrides=iter_overrides,\n",
    "    combine_overrides=combine_overrides,\n",
    "    global_config=global_config,\n",
    ")\n",
    "\n",
    "# This prints the config structures to the Users\n",
    "print(\"---- SyntheaConfig ----\")\n",
    "for config in config_handler.load_component_experimental_config(\"synthea\"):\n",
    "    print(config)\n",
    "\n",
    "print(\"\\n---- GenerateConfig ----\")\n",
    "for config in config_handler.load_component_experimental_config(\"generate\"):\n",
    "    print(config)\n",
    "\n",
    "print(\"\\n---- ExtractionConfig ----\")\n",
    "for config in config_handler.load_component_experimental_config(\"extraction\"):\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Prompt Templates used in the Experimental Pipeline.\n",
    "\n",
    "This pipeline uses three prompt templates that are required.\n",
    "* Uses a prompt template for **LLama2** which is used to generate the medical notes.\n",
    "* Uses a prompt template for **LLama3** which is used to generate the medical notes.\n",
    "* Uses a prompt template for UniversalNER which is used to extract the entities via both local and ollama.\n",
    "\n",
    "The below three cells define these templates and save them to a templates folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the path of where Llama2 template lives in the generate folder.\n",
    "generate_template_path = (\n",
    "    f\"{global_config.output_paths.generate_template}llama2_template.json\"\n",
    ")\n",
    "\n",
    "# This defines a template used by LLama2\n",
    "generate_template = \"\"\"[INST]\n",
    "<<SYS>>\n",
    "You are a medical student answering an exam question about writing clinical notes for patients.\n",
    "<</SYS>>\n",
    "\n",
    "Keep in mind that your answer will be accessed based on incorporating all the provided information and the quality of prose.\n",
    "\n",
    "1. Use prose to write an example clinical note for this patient's doctor.\n",
    "2. Use less than three sentences.\n",
    "3. Do not provide recommendations.\n",
    "4. Use the following information:\n",
    "\n",
    "{data}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Saves the template to the path defined.\n",
    "save_generate_template_to_json(\n",
    "    template_str=generate_template, file_path=generate_template_path\n",
    ")\n",
    "\n",
    "# Loads the template so the user can inspect the template saved.\n",
    "loaded_generate_template = load_and_validate_generate_prompt_template(\n",
    "    filename=generate_template_path\n",
    ")\n",
    "print(loaded_generate_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the path of where Llama3 template lives in the generate folder.\n",
    "generate_template_path = (\n",
    "    f\"{global_config.output_paths.generate_template}llama3_template.json\"\n",
    ")\n",
    "\n",
    "# This defines a template used by LLama3\n",
    "generate_template = \"\"\"<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are a medical student answering an exam question about writing clinical notes for patients.\n",
    "<|eot_id|>\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Keep in mind that your answer will be accessed based on incorporating all the provided information and the quality of prose.\n",
    "\n",
    "1. Use prose to write an example clinical note for this patient's doctor.\n",
    "2. Use less than three sentences.\n",
    "3. Do not provide recommendations.\n",
    "4. Use the following information:\n",
    "\n",
    "{data}\n",
    "<|eot_id|>\n",
    "\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Saves the template to the path defined.\n",
    "save_generate_template_to_json(\n",
    "    template_str=generate_template, file_path=generate_template_path\n",
    ")\n",
    "\n",
    "# Loads the template so the user can inspect the template saved.\n",
    "loaded_generate_template = load_and_validate_generate_prompt_template(\n",
    "    filename=generate_template_path\n",
    ")\n",
    "print(loaded_generate_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UniversalNER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_template_path = f\"{global_config.output_paths.extraction_template}universal_ner_template.json\"\n",
    "\n",
    "universalner_prompt_template = \"\"\"\n",
    "    USER: Text: {input_text}\n",
    "    ASSISTANT: I’ve read this text.\n",
    "    USER: What describes {entity_name} in the text?\n",
    "    ASSISTANT: (model's predictions in JSON format)\n",
    "    \"\"\"\n",
    "\n",
    "save_extraction_template_to_json(\n",
    "    template_str=universalner_prompt_template,\n",
    "    file_path=extraction_template_path,\n",
    ")\n",
    "\n",
    "loaded_extraction_template = load_and_validate_extraction_prompt_template(\n",
    "    filename=extraction_template_path\n",
    ")\n",
    "print(loaded_extraction_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GenerateSynthea: Generating Synthetic Patient Data using Synthea \n",
    "\n",
    "This extracts out all of the synthea defined configuration and then runs the configuration through the pipeline and saves the data to an ./example_output/example_experiment_name folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_handler.run_component_experiment_config(component_type=\"synthea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GenerateLLM: Generating Synthetic Patient Medical Notes \n",
    "\n",
    "This extracts out all of the generate defined configuration and then runs the configuration through the pipeline and saves the data to an ./example_output/experiment_name folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_handler.run_component_experiment_config(component_type=\"generate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction: Re-extracting Entities from the Patient Medical Notes\n",
    "\n",
    "This extracts out all of the extraction defined configuration and then runs the configuration through the pipeline and saves the data to an ./example_output/experiment_name folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_handler.run_component_experiment_config(component_type=\"extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualising the Experiment Workflow\n",
    "\n",
    "This method on the config handler allows a user to inspect their workflows data. This allows the user to get an idea of which configuration type runs into which output type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_handler.load_pipeline_visualisation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the Data\n",
    "\n",
    "By using the above workflow, you can then specify which data you would like to reload back into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_handler.load_specified_data_file(filename=\"extraction_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
